# -*- coding: utf-8 -*-
"""feature_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wXUeftymDCIxkrOSy8qU7KQQyWNferCB

# Explanation

The process of generating raw data can be found [here](https://docs.google.com/document/d/1beWVyDNKm_gAF08KsnXGxHyHfQdroD4PUQ27v8w5VNI/edit) 


I'll mainly explain how to transform the raw data into time series data:

### **The series_to_supervised() Function**

   We can use the shift() function in Pandas to automatically create new framings of time series problems given the desired length of input and output sequences.

 This would be a useful tool as it would allow us to explore different framings of a time series problem with machine learning algorithms to see which might result in better performing models.

In this section, we will define a new Python function named series_to_supervised() that takes a univariate or multivariate time series and frames it as a supervised learning dataset.

The function takes four arguments:

data: Sequence of observations as a list or 2D NumPy array. Required.
n_in: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.
n_out: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.
dropnan: Boolean whether or not to drop rows with NaN values. Optional. Defaults to True.
The function returns a single value:

return: Pandas DataFrame of series framed for supervised learning.
The new dataset is constructed as a DataFrame, with each column suitably named both by variable number and time step. This allows you to design a variety of different time step sequence type forecasting problems from a given univariate or multivariate time series.

Once the DataFrame is returned, you can decide how to split the rows of the returned DataFrame into X and y components for supervised learning any way you wish.

The function is defined with default parameters so that if you call it with just your data, it will construct a DataFrame with t-1 as X and t as y.

The function is confirmed to be compatible with Python 2 and Python 3.

The complete function is listed below, including function comments.
"""

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error

df=pd.read_excel('/bradjc5_cal_timeslot_full_nomissing_timeserie_extend.xlsx', header=0, index_col=0)
df.iloc[7935*2:7936*2,:]

df['pir'] = df['pir'].map(lambda x: 10 if x > 10 else x)

X=df[['Time','Day Status', 'Day','# of occupants', 'Location',
       'Activity','contact','pir','Weather',
       'High Temperature(F)', 'Low Temperature(F)', 'Pressure(inHg)',
       'Wind(mph)', 'Humidity(%)', 'MonDay']]

Y=df[['co2', 'concentration','humidity',
       'illumination','temperature', 'voc']]

var = ['High Temperature(F)', 'Low Temperature(F)', 'Pressure(inHg)', 'Wind(mph)', 'Humidity(%)']
for v in var[0:1]:
    plt.hist(X[v], np.linspace(min(X[v]),max(X[v]),100), histtype='bar', rwidth=0.8)

var = ['Time','Day Status', 'Day','# of occupants', 'Location', 'Activity','contact','pir','Weather']
for v in var[7:8]:
    plt.hist(X[v], np.linspace(min(X[v]),max(X[v]),100), histtype='bar', rwidth=0.8)

X['bin_High Temperature(F)'] = np.digitize(X['High Temperature(F)'], bins=np.linspace(30,90,13))
X['bin_Low Temperature(F)'] = np.digitize(X['Low Temperature(F)'], bins=np.linspace(30,90,13))
X['bin_Pressure(inHg)'] = np.digitize(X['Pressure(inHg)'], bins=np.linspace(29.7,30.4,15))
X['bin_Wind(mph)'] = np.digitize(X['Wind(mph)'], bins=np.linspace(0,20,21))
X['bin_Humidity(%)'] = np.digitize(X['Humidity(%)'], bins=np.linspace(0,100,51))
Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[-1,20,22,24,26,40])

X_bin = X.loc[:,['Time','Day Status', 'Day','# of occupants', 'Location',
       'Activity','contact','pir','Weather',
       'bin_High Temperature(F)', 'bin_Low Temperature(F)', 'bin_Pressure(inHg)',
       'bin_Wind(mph)', 'bin_Humidity(%)'
#                  ,'c_# of occupants_bin_High Temperature(F)'
#                  ,'c_# of occupants_bin_Low Temperature(F)'
#                  ,'c_# of occupants_Location'
#                  ,'c_Activity_Weather'
#                  ,'c_Activity_bin_High Temperature(F)'
#                  ,'c_Activity_bin_Low Temperature(F)'
#                  ,'c_Weather_bin_High Temperature(F)'
#                  ,'c_Weather_bin_Low Temperature(F)'
#                  ,'c_Weather_bin_Pressure(inHg)'
#                  ,'c_Weather_bin_Wind(mph)'
#                  ,'c_Weather_bin_Humidity(%)'
#                  ,'c_bin_High Temperature(F)_bin_Pressure(inHg)'
#                  ,'c_bin_Low Temperature(F)_bin_Pressure(inHg)'
#                  ,'c_bin_Humidity(%)_bin_High Temperature(F)'
#                  ,'c_bin_Humidity(%)_bin_Low Temperature(F)'
                ]]
[len( X.loc[:,cols].value_counts()) for cols in X.columns]
[len( X_bin.loc[:,cols].value_counts()) for cols in X_bin.columns]

ohe = OneHotEncoder()
ohe.fit(X_bin)
X_0 = ohe.transform(X_bin)
ohe.get_feature_names()

from sklearn.linear_model import LinearRegression,Lasso,LassoCV,LassoLarsCV
from sklearn.svm import SVR
from scipy.sparse import csr_matrix

train_split_size=0.8
prediction_targets = ['co2', 'concentration','humidity', 'illumination','temperature', 'voc']


for prediction_target in prediction_targets:

    print('predict ' + prediction_target +' :')
    
    if prediction_target == 'illumination':
        
        X_1=np.hstack([X_0.todense(),Y[prediction_target].values.reshape(-1,1)])
        X_2=X_1[(X_1[:,-1]>100).T.flat[:].tolist()[0],:]
        X_2=csr_matrix(X_2)
        
        train_sample_size = int(X_2.shape[0]*train_split_size)
        X_train = X_2[:train_sample_size,:-1]
        X_test = X_2[train_sample_size:,:-1]
        Y_train= X_2[:train_sample_size,-1]
        Y_test= X_2[train_sample_size:,-1]
        
        Y_train_= np.array(Y_train.todense().T.flat[:].tolist()[0])
        Y_test_=np.array(Y_test.todense().T.flat[:].tolist()[0])
        sample_weight = 1./Y_train_**2
        
    elif prediction_target == 'temperature':
        
        prediction_target = 'bin_' + prediction_target
        # [0,20,22,24,26,40]
        
        # split train/test set
        train_sample_size = int(X_0.shape[0]*train_split_size)
        X_train = X_0[:train_sample_size,:]
        X_test = X_0[train_sample_size:,:]
        Y_train = Y.iloc[:train_sample_size,:]
        Y_test = Y.iloc[train_sample_size:,:]

        Y_train_ = Y_train.loc[:,prediction_target].values[:]
        Y_test_ = Y_test.loc[:,prediction_target].values[:]
        sample_weight = 1./Y_train_**2
        
    else:
#         continue
        # split train/test set
        train_sample_size = int(X_0.shape[0]*train_split_size)
        X_train = X_0[:train_sample_size,:]
        X_test = X_0[train_sample_size:,:]
        Y_train = Y.iloc[:train_sample_size,:]
        Y_test = Y.iloc[train_sample_size:,:]
        
        smooth = 1
        Y_train_ = Y_train.loc[:,prediction_target].values[:] + smooth
        Y_test_ = Y_test.loc[:,prediction_target].values[:] + smooth
        sample_weight = 1./Y_train_**2
    
    models = ["""model = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1)"""
    #          ,"""model = Lasso(copy_X=True, alpha=0.002, selection='random', max_iter=2000)"""
    #          ,"""model = SVR(gamma='scale', C=10)"""
             ]


    

    

    for model_train in models:
        exec(model_train)
#         model.fit(X_train, Y_train_) 
        model.fit(X_train, Y_train_, sample_weight=sample_weight) 

        # make prediction
        Y_pred = model.predict(X_test)

        # compute RMSE
        RMSE = np.sqrt(mean_squared_error(Y_pred, Y_test_))
        # compute MAPE
        MAPE = sum(np.abs((Y_pred - Y_test_) / (Y_test_)) / len(Y_test_))
        print(model_train)
        print('RMSE:{:.3f}'.format(RMSE))
        print('MAPE:{:.3f}'.format(MAPE))
        print('\n')

"""Regression"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,Lasso,LassoCV,LassoLarsCV
from sklearn.svm import SVR
from scipy.sparse import csr_matrix

df=pd.read_excel('/bradjc5_cal_timeslot_full_nomissing_timeserie_extend.xlsx', header=0, index_col=0)
df['pir'] = df['pir'].map(lambda x: 10 if x > 10 else x)
X=df[['Time','Day Status', 'Day','# of occupants', 'Location',
       'Activity','contact','pir','Weather',
       'High Temperature(F)', 'Low Temperature(F)', 'Pressure(inHg)',
       'Wind(mph)', 'Humidity(%)', 'MonDay']]

Y=df[['co2', 'concentration','humidity',
       'illumination','temperature', 'voc']]
X['bin_High Temperature(F)'] = np.digitize(X['High Temperature(F)'], bins=np.linspace(30,90,13))
X['bin_Low Temperature(F)'] = np.digitize(X['Low Temperature(F)'], bins=np.linspace(30,90,13))
X['bin_Pressure(inHg)'] = np.digitize(X['Pressure(inHg)'], bins=np.linspace(29.7,30.4,15))
X['bin_Wind(mph)'] = np.digitize(X['Wind(mph)'], bins=np.linspace(0,20,21))
X['bin_Humidity(%)'] = np.digitize(X['Humidity(%)'], bins=np.linspace(0,100,51))



# Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[-1,20,22,24,26,40])
Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[0, 18, 22, 23.5, 26, 360])
Y['bin_co2'] = np.digitize(Y['co2'], bins=[0, 437, 456, 477, 548, 8990])
Y['bin_concentration'] = np.digitize(Y['concentration'], bins=[0, 573, 579, 603, 713, 11200])
Y['bin_humidity'] = np.digitize(Y['humidity'], bins=[0,42,53,66,74,1000])




Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[-1,20,22,24,26,40])
Y['bin_humidity'] = np.digitize(Y['humidity'], bins=[0,30,43,50,62,70,77])
Y['bin_voc'] = np.digitize(Y['voc'], bins=[0,500,700,800,900,1200])
X = pd.DataFrame(X, dtype='str')
X_bin = X.loc[:,['Time','Day Status', 'Day','# of occupants', 'Location',
       'Activity','contact','pir','Weather',
       'bin_High Temperature(F)', 'bin_Low Temperature(F)', 'bin_Pressure(inHg)',
       'bin_Wind(mph)', 'bin_Humidity(%)']]
ohe = OneHotEncoder()
ohe.fit(X_bin)
X_0 = ohe.transform(X_bin)
ohe.get_feature_names()

train_split_size=0.8
prediction_targets = ['co2', 'concentration','humidity', 'illumination','temperature', 'voc']


for prediction_target in prediction_targets:

    print('predict ' + prediction_target +' :')
    
    if prediction_target == 'illumination':
      
        X_1=np.hstack([X_0.todense(),Y[prediction_target].values.reshape(-1,1)])
        X_2=X_1[(X_1[:,-1]>100).T.flat[:].tolist()[0],:]
        X_2=csr_matrix(X_2)
        y = X_2[:,-1].todense().T.flat[:].tolist()[0]
        X_train, X_test, Y_train_, Y_test_ = train_test_split(X_2[:,:-1], np.array(y), test_size=0.2, random_state=42)
        sample_weight = 1./Y_train_**2
        
    elif prediction_target == 'temperature':
        
        prediction_target = 'bin_' + prediction_target
        y = Y.loc[:,prediction_target].values[:]
        X_train, X_test, Y_train_, Y_test_ = train_test_split(X_0, y, test_size=0.2, random_state=42)
        sample_weight = 1./Y_train_**2
        
        
    else:
        y = Y.loc[:,prediction_target].values[:]
        X_train, X_test, Y_train_, Y_test_ = train_test_split(X_0, y, test_size=0.2, random_state=42)
        sample_weight = 1./Y_train_**2
    
    models = ["""model = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1)"""
    #          ,"""model = Lasso(copy_X=True, alpha=0.002, selection='random', max_iter=2000)"""
    #          ,"""model = SVR(gamma='scale', C=10)"""
             ]

    for model_train in models:
        
        exec(model_train)
#         model.fit(X_train, Y_train_) 
        model.fit(X_train, Y_train_, sample_weight=sample_weight) 

        # make prediction
        Y_pred = model.predict(X_test)

        # compute RMSE
        RMSE = np.sqrt(mean_squared_error(Y_pred, Y_test_))
        # compute MAPE
        MAPE = sum(np.abs((Y_pred - Y_test_) / (Y_test_)) / len(Y_test_))
        print(model_train)
        print('RMSE:{:.3f}'.format(RMSE))
        print('MAPE:{:.3f}'.format(MAPE))
        print('\n')

Y_test_

import numpy as np
np.random.seed(1337)
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential 
from keras.layers import Dense,Activation 
from keras.optimizers import RMSprop 
## load 
(X_train,Y_train),(X_test,Y_test) = mnist.load_data() 
## normal
X_train = X_train.reshape(X_train.shape[0],-1)/255.
X_test = X_test.reshape(X_test.shape[0],-1)/255.
# Y_train = np_utils.to_categorical(Y_train,10)
# Y_test = np_utils.to_categorical(Y_test,10)

len(data2[4])



from keras.utils import np_utils

def genTrainTestSet(obj, clip, bins, cate_num):
  X_1=np.hstack([X_0.todense(),Y[obj].values.reshape(-1,1)])
  valid_rows = []
  for j in range(len(X_1[:,-1])):
#     if X_1[j,-1]>clip[0] and X_1[j,-1]<clip[1] and X_1[j,-1] not in clip:
      valid_rows.append(j)
  X_2 = X_1[valid_rows,:]
  y = X_2[:,-1].T.flat[:].tolist()[0]
  y = np.digitize(y, bins=bins)
  X_tot = X_2[:,:-1]
  X_train, X_test, Y_train_, Y_test_ = train_test_split(X_2[:,:-1], np.array(y), test_size=0.2, random_state=42)
  Y_train_= np_utils.to_categorical(Y_train_ - 1, cate_num)
  Y_test_= np_utils.to_categorical(Y_test_ - 1, cate_num)
  return X_train, X_test, Y_train_, Y_test_, X_tot

def genTrainTestSet2(obj, clip):
    X_1=np.hstack([X_0.todense(),Y[obj].values.reshape(-1,1)])
    valid_rows = []
    for j in range(len(X_1[:,-1])):
        if X_1[j,-1]>clip[0] and X_1[j,-1]<clip[1] and X_1[j,-1] not in clip:
            valid_rows.append(j)
    X_2 = X_1[valid_rows,:]
    y = X_2[:,-1].T.flat[:].tolist()[0]
    X_tot = X_2[:,:-1]
    X_train, X_test, Y_train_, Y_test_ = train_test_split(X_2[:,:-1], np.array(y), test_size=0.2, random_state=42)
    return X_train, X_test, Y_train_, Y_test_, X_tot

"""## Regression"""

from sklearn.linear_model import LinearRegression,Lasso,LassoCV,LassoLarsCV
from sklearn.svm import SVR
from scipy.sparse import csr_matrix
from sklearn import ensemble

train_split_size=0.8
prediction_targets = ['co2', 'concentration','humidity', 'illumination','temperature', 'voc']
# 1. 生成数据集
# class
# time series
data5 = genTrainTestSet2('voc', [300,10000])
# 305/30/10/5 acc=94%/99%
data1 = genTrainTestSet2('temperature', [15,36.8])
# 305/30/10/5 acc=85%/92%
data2 = genTrainTestSet2('co2', [400, 900])
# 305/30/10/5 acc=86%/93%
data6 = genTrainTestSet2('concentration', [350, 1120])
# 305/30/10/5 acc=77%/86%
data3 = genTrainTestSet2('humidity', [20, 80])
# 305/30/10/5 acc=75%/84%  
data4 = genTrainTestSet2('illumination', [200, 600])
# 305/30/10/5 acc=75%/84%  

data = [data1, data2, data3, data4, data5, data6]
mape = {}
models = [
            """model = LinearRegression(fit_intercept=True, n_jobs=1)"""
         ,"""model = Lasso(copy_X=True, alpha=0.002, selection='random', max_iter=2000)"""
         ,"""model = SVR(gamma='scale', C=10)"""
            ,"""model = ensemble.GradientBoostingRegressor(n_estimators=50)"""
         ]
for m in models:
  mape[m] = []

for d in data:

    X_train, X_test, Y_train_, Y_test_ = csr_matrix(d[0]), csr_matrix(d[1]), d[2], d[3]
    sample_weight = 1./Y_train_**2
    


    



    for model_train in models:
        
        
        
        exec(model_train)
        model.fit(X_train, Y_train_) 
#         model.fit(X_train, Y_train_, sample_weight=sample_weight) 

        # make prediction
        Y_pred = model.predict(X_test)

        # compute RMSE
        RMSE = np.sqrt(mean_squared_error(Y_pred, Y_test_))
        # compute MAPE
        MAPE = sum(np.abs((Y_pred - Y_test_) / (Y_test_+0.01)) / len(Y_test_))
        mape[model_train].append(MAPE)
        print(model_train)
        print('RMSE:{:.3f}'.format(RMSE))
        print('MAPE:{:.3f}'.format(MAPE))
        print('\n')



pred_result

"""## 3-layer DNN训练"""

import sys
import keras as K
import tensorflow as tf
from keras.utils import np_utils

py_ver = sys.version
k_ver = K.__version__
tf_ver = tf.__version__

print("Using Python version " + str(py_ver))
print("Using Keras version " + str(k_ver))
print("Using TensorFlow version " + str(tf_ver))

# Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[-1,20,22,24,26,40])
# Y['bin_voc'] = np.digitize(Y['voc'], bins=[0,500,700,800,900,1200])
# Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[0, 18, 22, 23.5, 26, 360])
# Y['bin_co2'] = np.digitize(Y['co2'], bins=[0, 437, 456, 477, 548, 8990])
# Y['bin_concentration'] = np.digitize(Y['concentration'], bins=[0, 573, 579, 603, 713, 11200])
# Y['bin_humidity'] = np.digitize(Y['humidity'], bins=[0,42,53,66,74,1000])


# 1. 生成数据集
# class
# time series
data1 = genTrainTestSet('voc', [279,1040], [405.8, 532.6, 659.4, 913], 5)
# 305/30/10/5 acc=94%/99%
data2 = genTrainTestSet('temperature', [0,36.8], [0, 18, 22, 25, 30], 5)
# 305/30/10/5 acc=85%/92%
data3 = genTrainTestSet('co2', [400, 900], [400, 500, 600, 700, 800], 5)
# 305/30/10/5 acc=86%/93%
data4 = genTrainTestSet('concentration', [350, 1120], [520, 650, 780, 910, 1040], 5)
# 305/30/10/5 acc=77%/86%
data5 = genTrainTestSet('humidity', [12.9, 100], [18, 28, 38, 48, 58], 5)
# 305/30/10/5 acc=75%/84%  
data6 = genTrainTestSet('illumination', [12.9, 100], [200, 300, 400, 500, 600], 5)
# 305/30/10/5 acc=75%/84%  




data = [ data1, data2, data3, data4, data5, data6 ]


# 2. 定义模型
init = K.initializers.glorot_uniform(seed=1)
simple_adam = K.optimizers.Adam()
model = K.models.Sequential()
# model.add(K.layers.Dense(units=30, input_dim=305, kernel_initializer=init, activation='sigmoid'))
# model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))
# model.add(K.layers.Dense(units=1, kernel_initializer=init, activation='softmax'))
model.add(K.layers.Dense(30, input_dim=305)) 
model.add(K.layers.Activation('relu')) 
model.add(K.layers.Dense(10, input_dim=30)) 
model.add(K.layers.Activation('relu')) 
model.add(K.layers.Dense(5, input_dim=10)) 
model.add(K.layers.Activation('softmax')) 
model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])
# model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])

# 3. 训练模型
b_size = 500
max_epochs = 200
acc=[]
loss=[]
pred_result = []

print("Starting training ")
for d in data[:]:
  model.fit(d[0], d[2], batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)
  loss2,accuracy = model.evaluate(d[1], d[3], batch_size = 500)
  pred_result.append(model.predict(d[4]))
  print ("loss: %s,accurcay: %s"%(loss,accuracy))
  acc.append(accuracy)
  loss.append(loss2)
  print("Training finished \n")


print(acc)



"""## SVM"""

def genTrainTestSet4SVM(obj, clip, bins, cate_num):
  X_1=np.hstack([X_0.todense(),Y[obj].values.reshape(-1,1)])
  valid_rows = []
  for j in range(len(X_1[:,-1])):
#     if X_1[j,-1]>clip[0] and X_1[j,-1]<clip[1] and X_1[j,-1] not in clip:
      valid_rows.append(j)
  X_2 = X_1[valid_rows,:]
  y = X_2[:,-1].T.flat[:].tolist()[0]
  y = np.digitize(y, bins=bins)
  X_tot = X_2[:,:-1]
  X_train, X_test, Y_train_, Y_test_ = train_test_split(X_2[:,:-1], np.array(y), test_size=0.2, random_state=42)
  return X_train, X_test, Y_train_, Y_test_, X_tot


from sklearn.svm import SVC
import sys
import keras as K
import tensorflow as tf
from keras.utils import np_utils

py_ver = sys.version
k_ver = K.__version__
tf_ver = tf.__version__

print("Using Python version " + str(py_ver))
print("Using Keras version " + str(k_ver))
print("Using TensorFlow version " + str(tf_ver))

# Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[-1,20,22,24,26,40])
# Y['bin_voc'] = np.digitize(Y['voc'], bins=[0,500,700,800,900,1200])
# Y['bin_temperature'] = np.digitize(Y['temperature'], bins=[0, 18, 22, 23.5, 26, 360])
# Y['bin_co2'] = np.digitize(Y['co2'], bins=[0, 437, 456, 477, 548, 8990])
# Y['bin_concentration'] = np.digitize(Y['concentration'], bins=[0, 573, 579, 603, 713, 11200])
# Y['bin_humidity'] = np.digitize(Y['humidity'], bins=[0,42,53,66,74,1000])


# 1. 生成数据集
# class
# time series
data1 = genTrainTestSet4SVM('voc', [279,1040], [405.8, 532.6, 659.4, 913], 5)
# 305/30/10/5 acc=94%/99%
data2 = genTrainTestSet4SVM('temperature', [0,36.8], [0, 18, 22, 25, 30], 5)
# 305/30/10/5 acc=85%/92%
data3 = genTrainTestSet4SVM('co2', [400, 900], [400, 500, 600, 700, 800], 5)
# 305/30/10/5 acc=86%/93%
data4 = genTrainTestSet4SVM('concentration', [350, 1120], [520, 650, 780, 910, 1040], 5)
# 305/30/10/5 acc=77%/86%
data5 = genTrainTestSet4SVM('humidity', [12.9, 100], [18, 28, 38, 48, 58], 5)
# 305/30/10/5 acc=75%/84%  
data6 = genTrainTestSet4SVM('illumination', [12.9, 100], [200, 300, 400, 500, 600], 5)
# 305/30/10/5 acc=75%/84%  




data = [ data1, data2, data3, data4, data5, data6 ]

for cc in [20, 50, 100]:
  model = SVC(C=cc)

  acc = []
  for d in data[:]:
    model.fit(d[0], d[2])
    pred_y = model.predict(d[1])
    acc.append(sum(pred_y-d[3] == 0)/len(d[3]))
  accc.append(acc)

accc

accc



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

ideal_predict_file_name='/bradjc5_cal_timeslot_full_nomissing.xlsx'
result_file_name='bradjc5_cal_timeslot_full_objective_all.csv'
save_com_ene_name='Fig12.com_ene_activity_all.png'
save_com_ene_name2='Fig11.com_ene_hour_all.png'
season='all'
width0,height=10,6


index_x = ['Call', 'Office hour', 'Preparation', 'Lunch',
           'Individual mt', 'Research', 'Rest', 'Weekly mt', 'Project mt',
           'Trip', 'Eating outside', 'Faculty mt', 'Teach', 'Running outside', 'Other']
index_num = ['1', '2', '4', '5', '6', '8', '9', '71', '72', '74', '75', '76', '77', '78', '79']

df=pd.read_excel(ideal_predict_file_name, header=0,index_col=0)
##data selection
df=df.drop(['Time','Location','Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1)
df.High_temperature = (df['High Temperature(F)']-32)/1.8
df.High_temperature.iloc[i]

df['pred_temperature']

"""## 生成预测结果"""

v0 = np.array([300, 450, 600, 750, 950]) # VOC/ ppb	& $<$405.8	& [405.8,532.6) & [532.6,659.4) & [659.4,913)	& $>$913\\ \hline
v1 = np.array([15, 18, 22, 25, 30]) # Temperature/$^\degree$C	& $<$18	& [18,22)	& [22,25)& [25, 30) &$>$30 \\ \hline
v2 = np.array([400, 500, 600, 700, 800]) # CO2/ppm	& $<$500	& [500,600) & [600,700) & [700,800)	& $>$800\\ \hline
v3 = np.array([600, 700, 850, 980, 1100]) # Concentration/ ppm	& $<$650	& [650,780) & [780,910) & [910,1040)	& $>$1040\\ \hline
v4 = np.array([25, 35, 45, 55, 65]) # Humidity/\%	& $<$28	& [28,38) &[38,48) & [48,58)	& $>$58 \\ \hline


f_name = '/bradjc5_cal_timeslot_full_nomissing.xlsx'
df=pd.read_excel(f_name, header=0,index_col=0)
df['pred_voc'] = [sum(i * v0) for i in pred_result[0]]
df['pred_temperature'] = [sum(i * v1) for i in pred_result[1]]
df['pred_co2'] = [sum(i * v2) for i in pred_result[2]]
df['pred_concentration'] = [sum(i * v3) for i in pred_result[3]]
df['pred_humidity'] = [sum(i * v4) for i in pred_result[4]]

mean_df

"""## energy & comfortability"""

season='fall'
fontsize = 24
linewidth=5

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

ideal_predict_file_name='/bradjc5_cal_timeslot_full_nomissing.xlsx'
result_file_name='bradjc5_cal_timeslot_full_objective_all.csv'
save_com_ene_name='Fig12.com_ene_activity_all.png'
save_com_ene_name2='Fig11.com_ene_hour_all.png'

width0,height=11,7.4

v0 = np.array([300, 450, 600, 750, 950]) # VOC/ ppb	& $<$405.8	& [405.8,532.6) & [532.6,659.4) & [659.4,913)	& $>$913\\ \hline
v1 = np.array([15, 18, 22, 25, 30]) # Temperature/$^\degree$C	& $<$18	& [18,22)	& [22,25)& [25, 30) &$>$30 \\ \hline
v2 = np.array([400, 500, 600, 700, 800]) # CO2/ppm	& $<$500	& [500,600) & [600,700) & [700,800)	& $>$800\\ \hline
v3 = np.array([600, 700, 850, 980, 1100]) # Concentration/ ppm	& $<$650	& [650,780) & [780,910) & [910,1040)	& $>$1040\\ \hline
v4 = np.array([25, 35, 45, 55, 65]) # Humidity/\%	& $<$28	& [28,38) &[38,48) & [48,58)	& $>$58 \\ \hline

df=pd.read_excel(ideal_predict_file_name, header=0,index_col=0)

if season == 'spring':
  print(season)

  # summer Somebody visit
  index_x = ['Calling', 'Office hour', 'Somebody visiting', 'Preparing for classes', 'Lunch time',
             'Individual meeting', 'Research activity', 'No real event', 'Weekly team meeting', 'Project meeting', 'Speaker',
             'Business Trip', 'Eating with visitor', 'Faculty meeting', 'Teaching']
  index_num = ['1', '2', '3', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']

  
  df=df.iloc[:7936,:]


  df['pred_voc'] = [sum(i * v0) for i in pred_result[0][:7936]]
  df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][:7936]]
  df['pred_co2'] = [sum(i * v2) for i in pred_result[2][:7936]]
  df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][:7936]]
  df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][:7936]]
elif season == 'summer':
  print(season)

  index_x = ['Calling', 'Office hour', 'Preparing for classes', 'Lunch time',
             'Individual meeting', 'Research activity', 'No real event', 'Weekly team meeting', 'Project meeting', 'Speaker',
             'Business Trip', 'Eating with visitor', 'Faculty meeting', 'Teaching']
  index_num = ['1', '2', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']

  df=df.iloc[7936:7936*2,:]


  df['pred_voc'] = [sum(i * v0) for i in pred_result[0][7936:7936*2]]
  df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][7936:7936*2]]
  df['pred_co2'] = [sum(i * v2) for i in pred_result[2][7936:7936*2]]
  df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][7936:7936*2]]
  df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][7936:7936*2]]
elif season == 'fall':
  print(season)
  # summer Somebody visit
  index_x = ['Calling', 'Office hour', 'Somebody visiting', 'Preparing for classes', 'Lunch time',
             'Individual meeting', 'Research activity', 'No real event', 'Weekly team meeting', 'Project meeting', 'Speaker',
             'Business Trip', 'Eating with visitor', 'Faculty meeting', 'Teaching']
  index_num = ['1', '2', '3', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']


  df=df.iloc[7936*2:,:]


  df['pred_voc'] = [sum(i * v0) for i in pred_result[0][7936*2:]]
  df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][7936*2:]]
  df['pred_co2'] = [sum(i * v2) for i in pred_result[2][7936*2:]]
  df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][7936*2:]]
  df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][7936*2:]]

  



morning=0
evening=95
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
timeslot=df['Time slot'].values
comf_improve_list=[]
energy_save_list=[]
hour_list=[]
for i in range(len(df)):
    hour_list.append(int(timeslot[i]/4))
    if df.Activity.iloc[i]<9:##inside
        ## comfortable, no need change:count=0
        energy_save = 0
        if (15<=df.pred_temperature.iloc[i]<=25 & 40<=df.pred_humidity.iloc[i]<=80 & df.illumination.iloc[i]>=300
            & df.pred_co2.iloc[i]<=600 & df.pred_concentration.iloc[i]>=700 & df.pred_concentration.iloc[i]<=900):

            comf_improve_list.append(0.000001)
            if df['High Temperature(F)'].iloc[i]>25*1.8+32:
                energy_tem_save=abs(float(df.pred_temperature.iloc[i]-25))/25.0
            elif df['Low Temperature(F)'].iloc[i]<15*1.8+32:
                energy_tem_save=abs(float(df.pred_temperature.iloc[i]-15))/15.0
            else:
                energy_tem_save=0
            energy_save+=energy_tem_save

            if df['Humidity(%)'].iloc[i]>80:
                energy_hum_save=abs(float(df.pred_humidity.iloc[i]-80))/80.0
            elif df['Humidity(%)'].iloc[i]<40:
                energy_hum_save=abs(float(df.pred_humidity.iloc[i]-40))/40.0
            else:
                energy_hum_save=0
            energy_save += energy_hum_save

            energy_illu_save=abs(float(df.illumination.iloc[i]-300))/300.0
            energy_save += energy_illu_save

            energy_air_save=min(abs(float(df.pred_co2.iloc[i]-600))/600.0,abs(float(df.pred_concentration.iloc[i]-700))/700.0,
            abs(float(df.pred_concentration.iloc[i]-900))/900.0)
            energy_save += energy_air_save

        else:
            comf_improve_list.append(1.0)
            if df['High Temperature(F)'].iloc[i]>25*1.8+32:
                energy_tem_use=abs(float(df.pred_temperature.iloc[i]-25))/25.0
            elif df['Low Temperature(F)'].iloc[i]<15*1.8+32:
                energy_tem_use=abs(float(df.pred_temperature.iloc[i]-15))/15.0
            else:
                if df.pred_temperature.iloc[i]-25>0:
                    energy_tem_use = abs(float(df.pred_temperature.iloc[i] - 25)) / 25.0
                else:
                    energy_tem_use = abs(float(df.pred_temperature.iloc[i] - 15)) / 15.0
            energy_save-=energy_tem_use

            if df['Humidity(%)'].iloc[i]>80:
                energy_hum_use=abs(float(df.pred_humidity.iloc[i]-80))/80.0
            elif df['Humidity(%)'].iloc[i]<40:
                energy_hum_use=abs(float(df.pred_humidity.iloc[i]-40))/40.0
            else:
                if df.pred_humidity.iloc[i]-80>0:
                    energy_hum_use = abs(float(df.pred_humidity.iloc[i] - 80)) / 80.0
                else:
                    energy_hum_use = abs(float(df.pred_humidity.iloc[i] - 40)) / 40.0
            energy_save -= energy_hum_use

            energy_illu_use=abs(float(df.illumination.iloc[i]-300))/300.0
            energy_save -= energy_illu_use

            energy_air_use=max(abs(float(df.pred_co2.iloc[i]-600))/600.0,abs(float(df.pred_concentration.iloc[i]-700))/700.0,
                               abs(float(df.pred_concentration.iloc[i]-900))/900.0)
            energy_save -= energy_air_use
        energy_save_list.append(energy_save)
    else:
        comf_improve_list.append(0)
        energy_save_list.append(4)
        
df['Comfotability_improving']=comf_improve_list
df['Energy_saving']=energy_save_list
df['Hour']=hour_list
df.to_csv(result_file_name, index=True)

##activity
mean_df=df.groupby('Activity').mean()
# print(mean_df.Comfotability_improving)
# print (mean_df.Energy_saving)

bar_width = 0.25
tick_label = index_x
x = np.arange(len(tick_label))

##set(0,0)
# ax = plt.gca()
# ax.spines['right'].set_color('none')
# ax.spines['top'].set_color('none')
# ax.xaxis.set_ticks_position('bottom')
# ax.yaxis.set_ticks_position('left')
# ax.spines['bottom'].set_position(('data', 0))
# ax.spines['left'].set_position(('data', 0))

# plt.rc('font', family='Times New Roman')
plt.figure(figsize=[width0,height+2.5])
bar1 = plt.plot(x, mean_df.Comfotability_improving, "o-", color="red", label="Comfortability", alpha=0.8, linewidth=linewidth)
# bar1 = plt.bar(x, mean_df.Comfotability_improving, bar_width, align="center", color="b", label="Comfotability_improving", alpha=0.5)
line3 = plt.plot(x,mean_df.Energy_saving,"o-", color="green",label="Energy saving", alpha=0.8, linewidth=linewidth)
plt.xlabel("Activity", fontsize=fontsize+5)
plt.ylabel("Units", fontsize=fontsize+5)
plt.xticks(x, tick_label,rotation=90,size=fontsize)
plt.yticks(size=fontsize)
color=['red','green']
labels = ["Comfortability","Energy saving"]
patches = [ mpatches.Patch(color=color[i], label="{:s}".format(labels[i]) ) for i in range(len(color)) ]
plt.legend(handles=patches, bbox_to_anchor=(0.85,1.3), ncol=2, fontsize=fontsize)
plt.ylim([-2.4,4.5])
plt.tight_layout()
plt.savefig(save_com_ene_name)
plt.show()

##2.hour
mean_df=df.groupby('Hour').mean()
print(mean_df.Comfotability_improving)
print (mean_df.Energy_saving)


bar_width = 0.25
tick_label = mean_df.index
print(tick_label)
x = np.arange(len(tick_label))
print(x)

##set(0,0)
# ax = plt.gca()
# ax.spines['right'].set_color('none')
# ax.spines['top'].set_color('none')
# ax.xaxis.set_ticks_position('bottom')
# ax.yaxis.set_ticks_position('left')
# ax.spines['bottom'].set_position(('data', 0))
# ax.spines['left'].set_position(('data', 0))

plt.rc('font', family='Times New Roman')
plt.figure(figsize=[width0,height-1.8])
bar1 = plt.plot(x, mean_df.Comfotability_improving, "o-", color="red", label="Comfortability", alpha=0.8, linewidth=linewidth)
line3 = plt.plot(x,mean_df.Energy_saving,"o-", color="green",label="Energy saving", alpha=0.8, linewidth=linewidth)
plt.xlabel("Hour", fontsize=fontsize+5)
plt.ylabel("Units", fontsize=fontsize+5)
plt.xticks(x, [ '0',  '',  '',  '',  '4',  '',  '',  '',  '8',  '', '', '', '12', '', '', '', '16',
            '', '', '', '20', '', '', '23'],size=fontsize)
plt.yticks(size=fontsize)
# plt.legend(handles=[bar1, bar2, line3], loc='best')
color=['red','green']
labels = ["Comfortability","Energy saving"]
patches = [ mpatches.Patch(color=color[i], label="{:s}".format(labels[i]) ) for i in range(len(color)) ]
plt.legend(handles=patches, bbox_to_anchor=(0.85,1.3), ncol=2,fontsize=fontsize)
# plt.legend(handles=patches, bbox_to_anchor=(0.85,1.3), ncol=4,fontsize=fontsize)
plt.ylim([-1.2,4.3])
plt.tight_layout()
plt.savefig(save_com_ene_name2)
plt.show()

"""# 新段落

# 新段落

# 新段落
"""

plt.plot?

"""## plot acc"""

model_names = ['Softmax', 'SVM', 'DNN', 'LSTM', 'Hybrid']
x_axix = ['Temp.', 'CO2', 'Humidity', 'Lighting','VOC', 'Concentration']
acc_softmax = [0.76, 0.89, 0.76, 0.97, 0.80, 0.85]
acc_svm = [0.79, 0.88, 0.78, 0.97, 0.82, 0.85]
acc_dnn = [0.84, 0.91, 0.82, 0.98, 0.92, 0.86]
acc_lstm=[0.84, 0.92, 0.84, 0.98, 0.97, 0.84]
acc_hybrid=[0.85, 0.92, 0.85, 0.99, 0.97, 0.86]

width0,height=10,6
total_width, n = 0.3, 2
plt.figure(figsize=[width0,height])
plt.rc('font',family='Times New Roman')
# plt.tight_layout()
plt.plot(x_axix, acc_softmax,"o-", color='purple', label=model_names[0])
plt.plot(x_axix, acc_svm, "o-",color='red', label=model_names[1])
plt.plot(x_axix, acc_dnn,"o-", color='blue', label=model_names[2])
plt.plot(x_axix, acc_lstm,"o-", color='orange', label=model_names[3])
plt.plot(x_axix, acc_hybrid, "o-",color='green', label=model_names[4])
# plt.legend(fontsize=14,loc='upper right')
plt.legend(loc='center left', bbox_to_anchor=(0.04, 1.15),ncol=3,fontsize=20)
plt.xlabel('Physical factors',size=22)
plt.ylabel('Accuracy',size=22)
plt.xticks(size=20)
plt.yticks(size=20)
plt.ylim(0.7, 1.0)
plt.tight_layout()
plt.savefig("class_acc.png")
plt.show()

mape

"""## plot MAPE"""

model_names = ['Linear Regression', 'LASSO', 'SVR', 'GBRT']
x_axix = ['Temp.', 'CO2', 'Humidity', 'Lighting','VOC', 'Concentration']
mape_LR = [0.06,0.06, 0.13, 0.08, 0.22, 0.06]
mape_LASSO = [0.05, 0.05, 0.13, 0.08, 0.21, 0.06]
mape_SVR = [0.03,0.04, 0.07, 0.10, 0.17, 0.05]
mape_GBRT=[0.07,0.06, 0.18, 0.11, 0.26, 0.07]


width0,height=10,6
total_width, n = 0.3, 2
plt.figure(figsize=[width0,height])
plt.rc('font',family='Times New Roman')
# plt.tight_layout()
plt.plot(x_axix, mape_LR,"o-", color='purple', label=model_names[0])
plt.plot(x_axix, mape_LASSO, "o-",color='red', label=model_names[1])
plt.plot(x_axix, mape_SVR,"o-", color='blue', label=model_names[2])
plt.plot(x_axix, mape_GBRT,"o-", color='orange', label=model_names[3])
# plt.legend(fontsize=14,loc='upper right')
plt.legend(loc='center left', bbox_to_anchor=(0.04, 1.15),ncol=3,fontsize=20)
plt.xlabel('Physical factors',size=22)
plt.ylabel('MAPE',size=22)
plt.xticks(size=20)
plt.yticks(size=20)
plt.ylim(0.0, .3)
plt.tight_layout()
plt.savefig("class_mape.png")
plt.show()

xx = []
xx2 = []
yy1 = []
yy2 = []
yy3 = []
yy4 = []
for season in ['spring', 'summer', 'fall']:
  fontsize = 24

  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import matplotlib.patches as mpatches

  ideal_predict_file_name='/bradjc5_cal_timeslot_full_nomissing.xlsx'
  result_file_name='bradjc5_cal_timeslot_full_objective_all.csv'
  save_com_ene_name='Fig12.com_ene_activity_all.png'
  save_com_ene_name2='Fig11.com_ene_hour_all.png'

  width0,height=11,7.4

  v0 = np.array([300, 450, 600, 750, 950]) # VOC/ ppb	& $<$405.8	& [405.8,532.6) & [532.6,659.4) & [659.4,913)	& $>$913\\ \hline
  v1 = np.array([15, 18, 22, 25, 30]) # Temperature/$^\degree$C	& $<$18	& [18,22)	& [22,25)& [25, 30) &$>$30 \\ \hline
  v2 = np.array([400, 500, 600, 700, 800]) # CO2/ppm	& $<$500	& [500,600) & [600,700) & [700,800)	& $>$800\\ \hline
  v3 = np.array([600, 700, 850, 980, 1100]) # Concentration/ ppm	& $<$650	& [650,780) & [780,910) & [910,1040)	& $>$1040\\ \hline
  v4 = np.array([25, 35, 45, 55, 65]) # Humidity/\%	& $<$28	& [28,38) &[38,48) & [48,58)	& $>$58 \\ \hline

  df=pd.read_excel(ideal_predict_file_name, header=0,index_col=0)

  if season == 'spring':
    print(season)

    # summer Somebody visit
    index_x = ['Call', 'Office hour', 'Somebody visit', 'Preparation', 'Lunch',
               'Individual mt', 'Research', 'No event', 'Weekly mt', 'Project mt', 'Speaker',
               'Trip', 'Eating outside', 'Faculty mt', 'Teach']
    index_num = ['1', '2', '3', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']


    df=df.iloc[:7936,:]


    df['pred_voc'] = [sum(i * v0) for i in pred_result[0][:7936]]
    df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][:7936]]
    df['pred_co2'] = [sum(i * v2) for i in pred_result[2][:7936]]
    df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][:7936]]
    df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][:7936]]
  elif season == 'summer':
    print(season)

    index_x = ['Call', 'Office hour', 'Preparation', 'Lunch',
               'Individual mt', 'Research', 'No event', 'Weekly mt', 'Project mt', 'Speaker',
               'Trip', 'Eating outside', 'Faculty mt', 'Teach']
    index_num = ['1', '2', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']

    df=df.iloc[7936:7936*2,:]


    df['pred_voc'] = [sum(i * v0) for i in pred_result[0][7936:7936*2]]
    df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][7936:7936*2]]
    df['pred_co2'] = [sum(i * v2) for i in pred_result[2][7936:7936*2]]
    df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][7936:7936*2]]
    df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][7936:7936*2]]
  elif season == 'fall':
    print(season)
    # summer Somebody visit
    index_x = ['Call', 'Office hour', 'Somebody visit', 'Preparation', 'Lunch',
               'Individual mt', 'Research', 'No event', 'Weekly mt', 'Project mt', 'Speaker',
               'Trip', 'Eating outside', 'Faculty mt', 'Teach']
    index_num = ['1', '2', '3', '4', '5', '6', '8', '9', '71', '72', '73', '74', '75', '76', '77']


    df=df.iloc[7936*2:,:]


    df['pred_voc'] = [sum(i * v0) for i in pred_result[0][7936*2:]]
    df['pred_temperature'] = [sum(i * v1) for i in pred_result[1][7936*2:]]
    df['pred_co2'] = [sum(i * v2) for i in pred_result[2][7936*2:]]
    df['pred_concentration'] = [sum(i * v3) for i in pred_result[3][7936*2:]]
    df['pred_humidity'] = [sum(i * v4) for i in pred_result[4][7936*2:]]





  morning=0
  evening=95
  daylight=evening-morning+1
  df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
  timeslot=df['Time slot'].values
  comf_improve_list=[]
  energy_save_list=[]
  hour_list=[]
  for i in range(len(df)):
      hour_list.append(int(timeslot[i]/4))
      if df.Activity.iloc[i]<9:##inside
          ## comfortable, no need change:count=0
          energy_save = 0
          if (15<=df.pred_temperature.iloc[i]<=25 & 40<=df.pred_humidity.iloc[i]<=80 & df.illumination.iloc[i]>=300
              & df.pred_co2.iloc[i]<=600 & df.pred_concentration.iloc[i]>=700 & df.pred_concentration.iloc[i]<=900):

              comf_improve_list.append(0.000001)
              if df['High Temperature(F)'].iloc[i]>25*1.8+32:
                  energy_tem_save=abs(float(df.pred_temperature.iloc[i]-25))/25.0
              elif df['Low Temperature(F)'].iloc[i]<15*1.8+32:
                  energy_tem_save=abs(float(df.pred_temperature.iloc[i]-15))/15.0
              else:
                  energy_tem_save=0
              energy_save+=energy_tem_save

              if df['Humidity(%)'].iloc[i]>80:
                  energy_hum_save=abs(float(df.pred_humidity.iloc[i]-80))/80.0
              elif df['Humidity(%)'].iloc[i]<40:
                  energy_hum_save=abs(float(df.pred_humidity.iloc[i]-40))/40.0
              else:
                  energy_hum_save=0
              energy_save += energy_hum_save

              energy_illu_save=abs(float(df.illumination.iloc[i]-300))/300.0
              energy_save += energy_illu_save

              energy_air_save=min(abs(float(df.pred_co2.iloc[i]-600))/600.0,abs(float(df.pred_concentration.iloc[i]-700))/700.0,
              abs(float(df.pred_concentration.iloc[i]-900))/900.0)
              energy_save += energy_air_save

          else:
              comf_improve_list.append(1.0)
              if df['High Temperature(F)'].iloc[i]>25*1.8+32:
                  energy_tem_use=abs(float(df.pred_temperature.iloc[i]-25))/25.0
              elif df['Low Temperature(F)'].iloc[i]<15*1.8+32:
                  energy_tem_use=abs(float(df.pred_temperature.iloc[i]-15))/15.0
              else:
                  if df.pred_temperature.iloc[i]-25>0:
                      energy_tem_use = abs(float(df.pred_temperature.iloc[i] - 25)) / 25.0
                  else:
                      energy_tem_use = abs(float(df.pred_temperature.iloc[i] - 15)) / 15.0
              energy_save-=energy_tem_use

              if df['Humidity(%)'].iloc[i]>80:
                  energy_hum_use=abs(float(df.pred_humidity.iloc[i]-80))/80.0
              elif df['Humidity(%)'].iloc[i]<40:
                  energy_hum_use=abs(float(df.pred_humidity.iloc[i]-40))/40.0
              else:
                  if df.pred_humidity.iloc[i]-80>0:
                      energy_hum_use = abs(float(df.pred_humidity.iloc[i] - 80)) / 80.0
                  else:
                      energy_hum_use = abs(float(df.pred_humidity.iloc[i] - 40)) / 40.0
              energy_save -= energy_hum_use

              energy_illu_use=abs(float(df.illumination.iloc[i]-300))/300.0
              energy_save -= energy_illu_use

              energy_air_use=max(abs(float(df.pred_co2.iloc[i]-600))/600.0,abs(float(df.pred_concentration.iloc[i]-700))/700.0,
                                 abs(float(df.pred_concentration.iloc[i]-900))/900.0)
              energy_save -= energy_air_use
          energy_save_list.append(energy_save)
      else:
          comf_improve_list.append(0)
          energy_save_list.append(4)

  df['Comfotability_improving']=comf_improve_list
  df['Energy_saving']=energy_save_list
  df['Hour']=hour_list
  df.to_csv(result_file_name, index=True)

  ##activity
  mean_df=df.groupby('Activity').mean()
  mean_df2=df.groupby('Hour').mean()
  # print(mean_df.Comfotability_improving)
  # print (mean_df.Energy_saving)

  bar_width = 0.25
  tick_label = index_x
  x = np.arange(len(tick_label))
  
  xx.append(x)
  yy1.append(mean_df.Comfotability_improving)
  yy2.append(mean_df.Energy_saving)
  
  yy3.append(mean_df2.Comfotability_improving)
  yy4.append(mean_df2.Energy_saving)
  
  
  
  tick_label2 = mean_df2.index
  x2 = np.arange(len(tick_label2))
  xx2.append(x2)
               
  


plt.figure(figsize=[width0,height])
bar1 = plt.plot(xx[0], yy1[0], "o-", color="blue", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx[0], yy2[0],"o-", color="red",label="Energy saving", alpha=0.8)

yy1[1] = np.concatenate([np.array(yy1[1])[:2], np.array([1]), np.array(yy1[1])[2:]])
yy2[1] = np.concatenate([np.array(yy2[1])[:2], np.array([-1.4]), np.array(yy2[1])[2:]])
bar1 = plt.plot(xx[2], yy1[1], "o-", color="purple", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx[2], yy2[1],"o-", color="black",label="Energy saving", alpha=0.8)

bar1 = plt.plot(xx[2], yy1[2], "o-", color="green", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx[2], yy2[2],"o-", color="orange",label="Energy saving", alpha=0.8)

plt.xlabel("Activity", fontsize=fontsize+5)
plt.ylabel("Units", fontsize=fontsize+5)
plt.xticks(x, tick_label, rotation=90,size=fontsize)
plt.yticks(size=20)
color=['b','lightgreen']
labels = ["Comfortability","Energy saving"]
patches = [ mpatches.Patch(color=color[i], label="{:s}".format(labels[i]) ) for i in range(len(color)) ]
plt.legend(handles=patches, bbox_to_anchor=(0.85,1.3), ncol=4, fontsize=fontsize)
plt.ylim([-3,4.2])
plt.tight_layout()
plt.savefig(save_com_ene_name)
plt.show()

##2.hour



##set(0,0)
# ax = plt.gca()
# ax.spines['right'].set_color('none')
# ax.spines['top'].set_color('none')
# ax.xaxis.set_ticks_position('bottom')
# ax.yaxis.set_ticks_position('left')
# ax.spines['bottom'].set_position(('data', 0))
# ax.spines['left'].set_position(('data', 0))

plt.rc('font', family='Times New Roman')
plt.figure(figsize=[width0,height])
bar1 = plt.plot(xx2[0], yy3[0], "o-", color="blue", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx2[0], yy4[0],"o-", color="red",label="Energy saving", alpha=0.8)

bar1 = plt.plot(xx2[1], yy3[1], "o-", color="purple", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx2[1], yy4[1],"o-", color="black",label="Energy saving", alpha=0.8)

bar1 = plt.plot(xx2[2], yy3[2], "o-", color="green", label="Comfortability", alpha=0.8)
line3 = plt.plot(xx2[2], yy4[2],"o-", color="orange",label="Energy saving", alpha=0.8)

plt.xlabel("Hour", fontsize=fontsize+5)
plt.ylabel("Units", fontsize=fontsize+5)
plt.xticks(x2, tick_label2,size=fontsize)
plt.yticks(size=20)
# plt.legend(handles=[bar1, bar2, line3], loc='best')
# color=['r','orange']
color=['b','lightgreen']
labels = ["Comfortability","Energy saving"]
patches = [ mpatches.Patch(color=color[i], label="{:s}".format(labels[i]) ) for i in range(len(color)) ]
plt.legend(handles=patches, bbox_to_anchor=(0.85,1.16), ncol=2,fontsize=fontsize)
# plt.legend(handles=patches, bbox_to_anchor=(0.85,1.3), ncol=4,fontsize=fontsize)
plt.ylim([-1.2,4])
plt.tight_layout()
plt.savefig(save_com_ene_name2)
plt.show()

np.concatenate([np.array(yy2[1])[:2], np.array([-1.4]), np.array(yy2[1])[2:]])

temperature_true = [int(6-sum(np.cumsum(i))) for i in data2[3]]
temperature_pred = [int(6-sum(np.cumsum(i))) for i in [i == max(i) for i in pred_result[1]]]

temperature_pred_diff = []
temperature_true_diff = []
for i in range(len(temperature_pred)-1):
  temperature_pred_diff.append(abs(temperature_pred[i]-temperature_pred[i+1]))
  temperature_true_diff.append(abs(temperature_true[i]-temperature_true[i+1]))

np.mean(temperature_true_diff)



sum([temperature_pred[i] == temperature_true[i] for i in range(len(temperature_true))]) / len(temperature_pred)

pd.Series(temperature_true_diff).value_counts()

pd.Series(temperature_pred_diff).value_counts()





acc

data2[0].shape

import sys
import keras as K
import tensorflow as tf
from keras.utils import np_utils

T = 14
seq_matrix = np.zeros((data2[0].shape[0], 14, data2[0].shape[1]))
for i in range(data2[0].shape[0]):
    for j in range(T):
        if i-T+1+j >= 0:
            seq_matrix[i, j, :] = data2[0][i-T+1+j, :]

seq_matrix.shape

optimizers.Adam?

"""## 实验"""

import keras
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
import numpy as np
from keras import optimizers
import random


T = 21
lstm_embedding_dim = 5
output_dim = 5
feature_dim = 305
random_feature_dim = 30
random_selected_feature = sorted(random.sample(range(feature_dim), random_feature_dim))

x = seq_matrix
main_input = Input(shape=(T, random_feature_dim + output_dim, ), dtype='float32', name='main_input')
lstm_out = LSTM(lstm_embedding_dim)(main_input)
auxiliary_input = Input(shape=(feature_dim,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])
x = Dense(30, activation='relu')(x)
x = Dense(10, activation='relu')(x)

main_output = Dense(5, activation='sigmoid', name='main_output')(x)

#生成模型
model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])
#编译模型
my_adam = optimizers.Adam(lr=0.001) #, decay=1e-3)
model.compile(loss='categorical_crossentropy', optimizer=my_adam, metrics=['accuracy'])
model.summary()


batch_size=500
epochs=150
loss1 = []
acc1 = []

for d in data[4:5]:
    seq_matrix_xy = np.zeros((d[0].shape[0], T, random_feature_dim + output_dim))
    seq_matrix_xy_eval = np.zeros((d[3].shape[0], T, random_feature_dim + output_dim))
    for i in range(d[0].shape[0]):
#         for j in range(T):
#             if i-T+1+j-1 >= 0:
#                 seq_matrix_xy[i, j, :] = np.hstack([d[0][i-T+1+j-1, random_selected_feature], d[2][i-T+1+j-1, :].reshape(1, output_dim)])
#             if i < d[3].shape[0] and i-T+1+j-1 >= 0:
#                 seq_matrix_xy_eval[i, j, :] = np.hstack([d[1][i-T+1+j-1, random_selected_feature], d[3][i-T+1+j-1, :].reshape(1, output_dim)])
   
        for j in range(T-1):
            if i-T+1+j >= 0:
                seq_matrix_xy[i, j, :] = np.hstack([d[0][i-T+1+j, random_selected_feature], d[2][i-T+1+j, :].reshape(1, output_dim)])
            if i < d[3].shape[0] and i-T+1+j >= 0:
                seq_matrix_xy_eval[i, j, :] = np.hstack([d[1][i-T+1+j, random_selected_feature], d[3][i-T+1+j, :].reshape(1, output_dim)])
        if i-1 >= 0:
            seq_matrix_xy[i, T-1, :] = np.hstack([d[0][i, random_selected_feature], d[2][i-1, :].reshape(1, output_dim)])
        if i < d[3].shape[0] and i-1 >= 0:
            seq_matrix_xy_eval[i, T-1, :] = np.hstack([d[1][i, random_selected_feature], d[3][i-1, :].reshape(1, output_dim)])
    # train 模型
    model.fit([seq_matrix_xy, d[0]], [d[2]], epochs=epochs, batch_size=batch_size)
    # eval 模型
    loss, accuracy = model.evaluate([seq_matrix_xy_eval, d[1]], [d[3]], batch_size = batch_size)
    loss1.append(loss)
    acc1.append(acc)
    print ("loss: %s,accurcay: %s"%(loss,accuracy))

predict_re

"""## 实验-fluctuation正则"""

import keras
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
import numpy as np
from keras import optimizers
import random


T = 21
lstm_embedding_dim = 5
output_dim = 5
feature_dim = 305

x = seq_matrix
main_input = Input(shape=(T, output_dim, ), dtype='float32', name='main_input')
lstm_out = LSTM(lstm_embedding_dim)(main_input)
auxiliary_output = Dense(output_dim, activation='sigmoid', name='aux_output')(lstm_out)
auxiliary_input = Input(shape=(feature_dim,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])
x = Dense(30, activation='relu')(x)
x = Dense(10, activation='relu')(x)

main_output = Dense(5, activation='sigmoid', name='main_output')(x)

#生成模型
model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])
#编译模型
my_adam = optimizers.Adam(lr=0.001) #, decay=1e-3)
model.compile(loss='categorical_crossentropy', optimizer=my_adam, metrics=['accuracy'],loss_weights=[1., 0.2])
model.summary()


batch_size = 500
epochs = 150
loss1 = []
acc1 = []

for d in data[1:2]:
    seq_matrix_xy = np.zeros((d[0].shape[0], T, output_dim))
    seq_matrix_xy_eval = np.zeros((d[3].shape[0], T, output_dim))
    for i in range(d[0].shape[0]):
        for j in range(T-1):
            if i-T+1+j >= 0:
                seq_matrix_xy[i, j, :] = d[2][i-T+1+j, :].reshape(1, output_dim)
            if i < d[3].shape[0] and i-T+1+j >= 0:
                seq_matrix_xy_eval[i, j, :] = d[3][i-T+1+j, :].reshape(1, output_dim)
        if i-1 >= 0:
            seq_matrix_xy[i, T-1, :] = d[2][i-1, :].reshape(1, output_dim)
        if i < d[3].shape[0] and i-1 >= 0:
            seq_matrix_xy_eval[i, T-1, :] = d[3][i-1, :].reshape(1, output_dim)
    # train 模型
    model.fit([seq_matrix_xy, d[0]], [d[2], d[2]], epochs=epochs, batch_size=batch_size)
    # eval 模型
    loss, accuracy = model.evaluate([seq_matrix_xy_eval, d[1]], [d[3], d[3]], batch_size = batch_size)
    loss1.append(loss)
    acc1.append(acc)
    pred_result.append(model.predict([seq_matrix_xy_eval, d[1]]))
    print ("loss: %s,accurcay: %s"%(loss,accuracy))

model.evaluate([seq_matrix_xy_eval, d[1]], [d[3], d[3]], batch_size = batch_size)



data2[2].shape

# -*- coding: utf-8 -*-
"""
Created on Fri Jan 12 10:28:27 2018
函数模型之多输入与多输出模型
@author: BruceWong
"""
import keras
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
import numpy as np
#generate data
#main_data
#这里构建数据集：主数据集为10000*100的二维数组，意味着100个特征
#标签为10000*1的二维数组，共有10种输出结果
main_x = np.random.random((10000,100))
main_y = keras.utils.to_categorical(np.random.randint(10,size = (10000,1)))
#additional_data
'''
All input arrays (x) should have the same number of samples. Got array shapes:
主数据集和额外的数据集的输入的特征张量的数据集个数相等，也就是行数相等；
'''
add_x = np.random.random((10000,10))
add_y = keras.utils.to_categorical(np.random.randint(10,size = (10000,1)))
# 设定主要输入的张量，并命名main_input
# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.
# Note that we can name any layer by passing it a "name" argument.
'''
所有的input里面的shape的维度均是特征个数维度，列的个数；shape=(特征个数,)
'''
main_input = Input(shape=(100,), dtype='int32', name='main_input')
# 嵌入生成512列维度的张量
# This embedding layer will encode the input sequence
# into a sequence of dense 512-dimensional vectors.
x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)
#print(x.shape)
#使用LSTM模型
# A LSTM will transform the vector sequence into a single vector,
# containing information about the entire sequence
lstm_out = LSTM(32)(x)
auxiliary_output = Dense(10, activation='sigmoid', name='aux_output')(lstm_out)

#额外的输入数据
'''
所有的input里面的shape的维度均是特征个数维度，列的个数；shape=(特征个数,)
'''
auxiliary_input = Input(shape=(10,), name='aux_input')
'''
#将LSTM得到的张量与额外输入的数据串联起来，这里是横向连接
'''
x = keras.layers.concatenate([lstm_out, auxiliary_input])
#建立一个深层连接的网络
# We stack a deep densely-connected network on top
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

#得到主数据集输出的张量，10与输入的主数据集标签数据集的标签类相等
# And finally we add the main logistic regression layer
main_output = Dense(10, activation='sigmoid', name='main_output')(x)

#生成模型
model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])
#编译模型
model.compile(optimizer='rmsprop', loss='binary_crossentropy',loss_weights=[1., 0.2])
#训练模型
model.fit([main_x, add_x], [main_y, main_y], epochs=10, batch_size=128)

#model.compile(optimizer='rmsprop',loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},loss_weights={'main_output': 1., 'aux_output': 0.2})

# And trained it via:
#model.fit({'main_input': headline_data, 'aux_input': additional_data},{'main_output': labels, 'aux_output': labels},epochs=50, batch_size=32)

# -*- coding: utf-8 -*-
"""
Created on Fri Jan 12 10:28:27 2018
函数模型之多输入与多输出模型
@author: BruceWong
"""
import keras
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
import numpy as np
#generate data
#main_data
#这里构建数据集：主数据集为10000*100的二维数组，意味着100个特征
#标签为10000*1的二维数组，共有10种输出结果
main_x = np.random.random((10000,100))
main_y = keras.utils.to_categorical(np.random.randint(10,size = (10000,1)))
#additional_data
'''
All input arrays (x) should have the same number of samples. Got array shapes:
主数据集和额外的数据集的输入的特征张量的数据集个数相等，也就是行数相等；
'''
add_x = np.random.random((10000,10))
add_y = keras.utils.to_categorical(np.random.randint(10,size = (10000,1)))
# 设定主要输入的张量，并命名main_input
# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.
# Note that we can name any layer by passing it a "name" argument.
'''
所有的input里面的shape的维度均是特征个数维度，列的个数；shape=(特征个数,)
'''
main_input = Input(shape=(100,), dtype='int32', name='main_input')
# 嵌入生成512列维度的张量
# This embedding layer will encode the input sequence
# into a sequence of dense 512-dimensional vectors.
x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)
#print(x.shape)
#使用LSTM模型
# A LSTM will transform the vector sequence into a single vector,
# containing information about the entire sequence
lstm_out = LSTM(32)(x)
auxiliary_output = Dense(10, activation='sigmoid', name='aux_output')(lstm_out)

#额外的输入数据
'''
所有的input里面的shape的维度均是特征个数维度，列的个数；shape=(特征个数,)
'''
auxiliary_input = Input(shape=(10,), name='aux_input')
'''
#将LSTM得到的张量与额外输入的数据串联起来，这里是横向连接
'''
x = keras.layers.concatenate([lstm_out, auxiliary_input])
#建立一个深层连接的网络
# We stack a deep densely-connected network on top
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

#得到主数据集输出的张量，10与输入的主数据集标签数据集的标签类相等
# And finally we add the main logistic regression layer
main_output = Dense(10, activation='sigmoid', name='main_output')(x)

#生成模型
model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])
#编译模型
model.compile(optimizer='rmsprop', loss='binary_crossentropy',loss_weights=[1., 0.2])
#训练模型
model.fit([main_x, add_x], [main_y, main_y],epochs=10, batch_size=128)

#model.compile(optimizer='rmsprop',loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},loss_weights={'main_output': 1., 'aux_output': 0.2})

# And trained it via:
#model.fit({'main_input': headline_data, 'aux_input': additional_data},{'main_output': labels, 'aux_output': labels},epochs=50, batch_size=32)

acc

Y_train.loc[:,'bin_temperature'].values[:] -1

max(Y_train_)

Y_train_

max(Y['concentration'])

































from pandas import DataFrame
from pandas import concat

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	"""
	Frame a time series as a supervised learning dataset.
	Arguments:
		data: Sequence of observations as a list or NumPy array.
		n_in: Number of lag observations as input (X).
		n_out: Number of observations as output (y).
		dropnan: Boolean whether or not to drop rows with NaN values.
	Returns:
		Pandas DataFrame of series framed for supervised learning.
	"""
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

"""# import"""

# from feature_selector import FeatureSelector

import pandas as pd
from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error,mean_absolute_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import GRU, Dropout
import datetime
from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import BayesianRidge, LinearRegression, ElasticNet
from sklearn.svm import SVR
from sklearn.utils.class_weight import compute_class_weight

import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

"""# feature_selector.py"""

# numpy and pandas for data manipulation
import pandas as pd
import numpy as np

# model used for feature importances
import lightgbm as lgb

# utility for early stopping with a validation set
from sklearn.model_selection import train_test_split

# visualizations
import matplotlib.pyplot as plt
import seaborn as sns

# memory management
import gc

# utilities
from itertools import chain

class FeatureSelector():
    """
    Class for performing feature selection for machine learning or data preprocessing.
    
    Implements five different methods to identify features for removal 
    
        1. Find columns with a missing percentage greater than a specified threshold
        2. Find columns with a single unique value
        3. Find collinear variables with a correlation greater than a specified correlation coefficient
        4. Find features with 0.0 feature importance from a gradient boosting machine (gbm)
        5. Find low importance features that do not contribute to a specified cumulative feature importance from the gbm
        
    Parameters
    --------
        data : dataframe
            A dataset with observations in the rows and features in the columns

        labels : array or series, default = None
            Array of labels for training the machine learning model to find feature importances. These can be either binary labels
            (if task is 'classification') or continuous targets (if task is 'regression').
            If no labels are provided, then the feature importance based methods are not available.
        
    Attributes
    --------
    
    ops : dict
        Dictionary of operations run and features identified for removal
        
    missing_stats : dataframe
        The fraction of missing values for all features
    
    record_missing : dataframe
        The fraction of missing values for features with missing fraction above threshold
        
    unique_stats : dataframe
        Number of unique values for all features
    
    record_single_unique : dataframe
        Records the features that have a single unique value
        
    corr_matrix : dataframe
        All correlations between all features in the data
    
    record_collinear : dataframe
        Records the pairs of collinear variables with a correlation coefficient above the threshold
        
    feature_importances : dataframe
        All feature importances from the gradient boosting machine
    
    record_zero_importance : dataframe
        Records the zero importance features in the data according to the gbm
    
    record_low_importance : dataframe
        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm
    
    
    Notes
    --------
    
        - All 5 operations can be run with the `identify_all` method.
        - If using feature importances, one-hot encoding is used for categorical variables which creates new columns
    
    """
    
    def __init__(self, data, labels=None):
        
        # Dataset and optional training labels
        self.data = data
        self.labels = labels

        if labels is None:
            print('No labels provided. Feature importance based methods are not available.')
        
        self.base_features = list(data.columns)
        self.one_hot_features = None
        
        # Dataframes recording information about features to remove
        self.record_missing = None
        self.record_single_unique = None
        self.record_collinear = None
        self.record_zero_importance = None
        self.record_low_importance = None
        
        self.missing_stats = None
        self.unique_stats = None
        self.corr_matrix = None
        self.feature_importances = None
        
        # Dictionary to hold removal operations
        self.ops = {}
        
        self.one_hot_correlated = False
        
    def identify_missing(self, missing_threshold):
        """Find the features with a fraction of missing values above `missing_threshold`"""
        
        self.missing_threshold = missing_threshold

        # Calculate the fraction of missing in each column 
        missing_series = self.data.isnull().sum() / self.data.shape[0]
        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})

        # Sort with highest number of missing values on top
        self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)

        # Find the columns with a missing percentage above the threshold
        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = 
                                                                                                               {'index': 'feature', 
                                                                                                                0: 'missing_fraction'})

        to_drop = list(record_missing['feature'])

        self.record_missing = record_missing
        self.ops['missing'] = to_drop
        
        print('%d features with greater than %0.2f missing values.\n' % (len(self.ops['missing']), self.missing_threshold))
        
    def identify_single_unique(self):
        """Finds features with only a single unique value. NaNs do not count as a unique value. """

        # Calculate the unique counts in each column
        unique_counts = self.data.nunique()
        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})
        self.unique_stats = self.unique_stats.sort_values('nunique', ascending = True)
        
        # Find the columns with only one unique count
        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', 
                                                                                                                0: 'nunique'})

        to_drop = list(record_single_unique['feature'])
    
        self.record_single_unique = record_single_unique
        self.ops['single_unique'] = to_drop
        
        print('%d features with a single unique value.\n' % len(self.ops['single_unique']))
    
    def identify_collinear(self, correlation_threshold, one_hot=False):
        """
        Finds collinear features based on the correlation coefficient between features. 
        For each pair of features with a correlation coefficient greather than `correlation_threshold`,
        only one of the pair is identified for removal. 

        Using code adapted from: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/
        
        Parameters
        --------

        correlation_threshold : float between 0 and 1
            Value of the Pearson correlation cofficient for identifying correlation features

        one_hot : boolean, default = False
            Whether to one-hot encode the features before calculating the correlation coefficients

        """
        
        self.correlation_threshold = correlation_threshold
        self.one_hot_correlated = one_hot
        
         # Calculate the correlations between every column
        if one_hot:
            
            # One hot encoding
            features = pd.get_dummies(self.data)
            self.one_hot_features = [column for column in features.columns if column not in self.base_features]

            # Add one hot encoded data to original data
            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)
            
            corr_matrix = pd.get_dummies(features).corr()

        else:
            corr_matrix = self.data.corr()
        
        self.corr_matrix = corr_matrix
    
        # Extract the upper triangle of the correlation matrix
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))
        
        # Select the features with correlations above the threshold
        # Need to use the absolute value
        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]

        # Dataframe to hold correlated pairs
        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])

        # Iterate through the columns to drop to record pairs of correlated features
        for column in to_drop:

            # Find the correlated features
            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])

            # Find the correlated values
            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])
            drop_features = [column for _ in range(len(corr_features))]    

            # Record the information (need a temp df for now)
            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,
                                             'corr_feature': corr_features,
                                             'corr_value': corr_values})

            # Add to dataframe
            record_collinear = record_collinear.append(temp_df, ignore_index = True)

        self.record_collinear = record_collinear
        self.ops['collinear'] = to_drop
        
        print('%d features with a correlation magnitude greater than %0.2f.\n' % (len(self.ops['collinear']), self.correlation_threshold))

    def identify_zero_importance(self, task, eval_metric=None, 
                                 n_iterations=10, early_stopping = True):
        """
        
        Identify the features with zero importance according to a gradient boosting machine.
        The gbm can be trained with early stopping using a validation set to prevent overfitting. 
        The feature importances are averaged over `n_iterations` to reduce variance. 
        
        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)

        Parameters 
        --------

        eval_metric : string
            Evaluation metric to use for the gradient boosting machine for early stopping. Must be
            provided if `early_stopping` is True

        task : string
            The machine learning task, either 'classification' or 'regression'

        n_iterations : int, default = 10
            Number of iterations to train the gradient boosting machine
            
        early_stopping : boolean, default = True
            Whether or not to use early stopping with a validation set when training
        
        
        Notes
        --------
        
        - Features are one-hot encoded to handle the categorical variables before training.
        - The gbm is not optimized for any particular task and might need some hyperparameter tuning
        - Feature importances, including zero importance features, can change across runs

        """

        if early_stopping and eval_metric is None:
            raise ValueError("""eval metric must be provided with early stopping. Examples include "auc" for classification or
                             "l2" for regression.""")
            
        if self.labels is None:
            raise ValueError("No training labels provided.")
        
        # One hot encoding
        features = pd.get_dummies(self.data)
        self.one_hot_features = [column for column in features.columns if column not in self.base_features]

        # Add one hot encoded data to original data
        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)

        # Extract feature names
        feature_names = list(features.columns)

        # Convert to np array
        features = np.array(features)
        labels = np.array(self.labels).reshape((-1, ))

        # Empty array for feature importances
        feature_importance_values = np.zeros(len(feature_names))
        
        print('Training Gradient Boosting Model\n')
        
        # Iterate through each fold
        for _ in range(n_iterations):

            if task == 'classification':
                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            elif task == 'regression':
                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            else:
                raise ValueError('Task must be either "classification" or "regression"')
                
            # If training using early stopping need a validation set
            if early_stopping:
                
                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)

                # Train the model with early stopping
                model.fit(train_features, train_labels, eval_metric = eval_metric,
                          eval_set = [(valid_features, valid_labels)],
                          early_stopping_rounds = 100, verbose = -1)
                
                # Clean up memory
                gc.enable()
                del train_features, train_labels, valid_features, valid_labels
                gc.collect()
                
            else:
                model.fit(features, labels)

            # Record the feature importances
            feature_importance_values += model.feature_importances_ / n_iterations

        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})

        # Sort features according to importance
        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)

        # Normalize the feature importances to add up to one
        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()
        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])

        # Extract the features with zero importance
        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]
        
        to_drop = list(record_zero_importance['feature'])

        self.feature_importances = feature_importances
        self.record_zero_importance = record_zero_importance
        self.ops['zero_importance'] = to_drop
        
        print('\n%d features with zero importance after one-hot encoding.\n' % len(self.ops['zero_importance']))
    
    def identify_low_importance(self, cumulative_importance):
        """
        Finds the lowest importance features not needed to account for `cumulative_importance` fraction
        of the total feature importance from the gradient boosting machine. As an example, if cumulative
        importance is set to 0.95, this will retain only the most important features needed to 
        reach 95% of the total feature importance. The identified features are those not needed.

        Parameters
        --------
        cumulative_importance : float between 0 and 1
            The fraction of cumulative importance to account for 

        """

        self.cumulative_importance = cumulative_importance
        
        # The feature importances need to be calculated before running
        if self.feature_importances is None:
            raise NotImplementedError("""Feature importances have not yet been determined. 
                                         Call the `identify_zero_importance` method first.""")
            
        # Make sure most important features are on top
        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')

        # Identify the features not needed to reach the cumulative_importance
        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]

        to_drop = list(record_low_importance['feature'])

        self.record_low_importance = record_low_importance
        self.ops['low_importance'] = to_drop
    
        print('%d features required for cumulative importance of %0.2f after one hot encoding.' % (len(self.feature_importances) -
                                                                            len(self.record_low_importance), self.cumulative_importance))
        print('%d features do not contribute to cumulative importance of %0.2f.\n' % (len(self.ops['low_importance']),
                                                                                               self.cumulative_importance))
        
    def identify_all(self, selection_params):
        """
        Use all five of the methods to identify features to remove.
        
        Parameters
        --------
            
        selection_params : dict
           Parameters to use in the five feature selection methhods.
           Params must contain the keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']
        
        """
        
        # Check for all required parameters
        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:
            if param not in selection_params.keys():
                raise ValueError('%s is a required parameter for this method.' % param)
        
        # Implement each of the five methods
        self.identify_missing(selection_params['missing_threshold'])
        self.identify_single_unique()
        self.identify_collinear(selection_params['correlation_threshold'])
        self.identify_zero_importance(task = selection_params['task'], eval_metric = selection_params['eval_metric'])
        self.identify_low_importance(selection_params['cumulative_importance'])
        
        # Find the number of features identified to drop
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        self.n_identified = len(self.all_identified)
        
        print('%d total features out of %d identified for removal after one-hot encoding.\n' % (self.n_identified, 
                                                                                                  self.data_all.shape[1]))
        
    def check_removal(self, keep_one_hot=True):
        
        """Check the identified features before removal. Returns a list of the unique features identified."""
        
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        print('Total of %d features identified for removal' % len(self.all_identified))
        
        if not keep_one_hot:
            if self.one_hot_features is None:
                print('Data has not been one-hot encoded')
            else:
                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]
                print('%d additional one-hot features can be removed' % len(one_hot_to_remove))
        
        return list(self.all_identified)
        
    
    def remove(self, methods, keep_one_hot = True):
        """
        Remove the features from the data according to the specified methods.
        
        Parameters
        --------
            methods : 'all' or list of methods
                If methods == 'all', any methods that have identified features will be used
                Otherwise, only the specified methods will be used.
                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']
            keep_one_hot : boolean, default = True
                Whether or not to keep one-hot encoded features
                
        Return
        --------
            data : dataframe
                Dataframe with identified features removed
                
        
        Notes 
        --------
            - If feature importances are used, the one-hot encoded columns will be added to the data (and then may be removed)
            - Check the features that will be removed before transforming data!
        
        """
        
        
        features_to_drop = []
      
        if methods == 'all':
            
            # Need to use one-hot encoded data as well
            data = self.data_all
                                          
            print('{} methods have been run\n'.format(list(self.ops.keys())))
            
            # Find the unique features to drop
            features_to_drop = set(list(chain(*list(self.ops.values()))))
            
        else:
            # Need to use one-hot encoded data as well
            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:
                data = self.data_all
                
            else:
                data = self.data
                
            # Iterate through the specified methods
            for method in methods:
                
                # Check to make sure the method has been run
                if method not in self.ops.keys():
                    raise NotImplementedError('%s method has not been run' % method)
                    
                # Append the features identified for removal
                else:
                    features_to_drop.append(self.ops[method])
        
            # Find the unique features to drop
            features_to_drop = set(list(chain(*features_to_drop)))
            
        features_to_drop = list(features_to_drop)
            
        if not keep_one_hot:
            
            if self.one_hot_features is None:
                print('Data has not been one-hot encoded')
            else:
                             
                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))
       
        # Remove the features and return the data
        data = data.drop(columns = features_to_drop)
        self.removed_features = features_to_drop
        
        if not keep_one_hot:
        	print('Removed %d features including one-hot features.' % len(features_to_drop))
        else:
        	print('Removed %d features.' % len(features_to_drop))
        
        return data
    
    def plot_missing(self):
        """Histogram of missing fraction in each feature"""
        if self.record_missing is None:
            raise NotImplementedError("Missing values have not been calculated. Run `identify_missing`")
        
        self.reset_plot()
        
        # Histogram of missing values
        plt.style.use('seaborn-white')
        plt.figure(figsize = (7, 5))
        plt.hist(self.missing_stats['missing_fraction'], bins = np.linspace(0, 1, 11), edgecolor = 'k', color = 'red', linewidth = 1.5)
        plt.xticks(np.linspace(0, 1, 11));
        plt.xlabel('Missing Fraction', size = 14); plt.ylabel('Count of Features', size = 14); 
        plt.title("Fraction of Missing Values Histogram", size = 16);
        
    
    def plot_unique(self):
        """Histogram of number of unique values in each feature"""
        if self.record_single_unique is None:
            raise NotImplementedError('Unique values have not been calculated. Run `identify_single_unique`')
        
        self.reset_plot()

        # Histogram of number of unique values
        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (7, 5))
        plt.ylabel('Frequency', size = 14); plt.xlabel('Unique Values', size = 14); 
        plt.title('Number of Unique Values Histogram', size = 16);
        
    
    def plot_collinear(self, plot_all = False):
        """
        Heatmap of the correlation values. If plot_all = True plots all the correlations otherwise
        plots only those features that have a correlation above the threshold
        
        Notes
        --------
            - Not all of the plotted correlations are above the threshold because this plots
            all the variables that have been idenfitied as having even one correlation above the threshold
            - The features on the x-axis are those that will be removed. The features on the y-axis
            are the correlated features with those on the x-axis
        
        Code adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html
        """
        
        if self.record_collinear is None:
            raise NotImplementedError('Collinear features have not been idenfitied. Run `identify_collinear`.')
        
        if plot_all:
        	corr_matrix_plot = self.corr_matrix
        	title = 'All Correlations'
        
        else:
	        # Identify the correlations that were above the threshold
	        # columns (x-axis) are features to drop and rows (y_axis) are correlated pairs
	        corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])), 
	                                                list(set(self.record_collinear['drop_feature']))]

	        title = "Correlations Above Threshold"

       
        f, ax = plt.subplots(figsize=(10, 8))
        
        # Diverging colormap
        cmap = sns.diverging_palette(220, 10, as_cmap=True)

        # Draw the heatmap with a color bar
        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,
                    linewidths=.25, cbar_kws={"shrink": 0.6})

        # Set the ylabels 
        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])
        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));

        # Set the xlabels 
        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])
        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));
        plt.title(title, size = 14)
        
    def plot_feature_importances(self, plot_n = 15, threshold = None):
        """
        Plots `plot_n` most important features and the cumulative importance of features.
        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.

        Parameters
        --------
        
        plot_n : int, default = 15
            Number of most important features to plot. Defaults to 15 or the maximum number of features whichever is smaller
        
        threshold : float, between 0 and 1 default = None
            Threshold for printing information about cumulative importances

        """
        
        if self.record_zero_importance is None:
            raise NotImplementedError('Feature importances have not been determined. Run `idenfity_zero_importance`')
            
        # Need to adjust number of features if greater than the features in the data
        if plot_n > self.feature_importances.shape[0]:
            plot_n = self.feature_importances.shape[0] - 1

        self.reset_plot()
        
        # Make a horizontal bar chart of feature importances
        plt.figure(figsize = (10, 6))
        ax = plt.subplot()

        # Need to reverse the index to plot most important on top
        # There might be a more efficient method to accomplish this
        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))), 
                self.feature_importances['normalized_importance'][:plot_n], 
                align = 'center', edgecolor = 'k')

        # Set the yticks and labels
        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))
        ax.set_yticklabels(self.feature_importances['feature'][:plot_n], size = 12)

        # Plot labeling
        plt.xlabel('Normalized Importance', size = 16); plt.title('Feature Importances', size = 18)
        plt.show()

        # Cumulative importance plot
        plt.figure(figsize = (6, 4))
        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')
        plt.xlabel('Number of Features', size = 14); plt.ylabel('Cumulative Importance', size = 14); 
        plt.title('Cumulative Feature Importance', size = 16);

        if threshold:

            # Index of minimum number of features needed for cumulative importance threshold
            # np.where returns the index so need to add 1 to have correct number
            importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))
            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles='--', colors = 'blue')
            plt.show();

            print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))

    def reset_plot(self):
        plt.rcParams = plt.rcParamsDefault

"""# functions"""

def mean_absolute_percent_error(true, pred):
    diff = np.abs(np.array(true) - np.array(pred))
    return np.mean(diff / true)


# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

"""# feature_extraction"""

# load dataset
# dataset = read_csv('pollution.csv', header=0, index_col=0)
# values = dataset.values

###change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
# save_png1='mape_fall.png'
# save_png2='rmse_fall.png'
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


##no replacing
if season=='summer_3':pf_names = ['Temperature','Humidity','Concentration']
else:pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
names =np.array(['LSTM'])
model_names=np.tile(names,len(pf_names)).tolist()




df = read_csv(read_name, header=0, index_col=0)
morning=40
evening=65
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
values = df.drop(['Time','Location','Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1).values
# print(values)
# print(values[:, 4])
# # integer encode direction
# encoder = LabelEncoder()
# values[:, 4] = encoder.fit_transform(values[:, 4])#Activity
# #ensure all data is float
values = values.astype('float32')

# #normalize features
scaler = MinMaxScaler(feature_range=(0.000001, 1))
scaled = scaler.fit_transform(values)

print('-----SIZE OF VALUES-------')
print(DataFrame(values).shape)
print(DataFrame(scaled).shape)

# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv(save_name, index=True)

###20Activity
model_metrics=[]
p=0
if season=='summer_3':pf_col=[27,24,22]##summer->list out of range
else:pf_col=[27,24,25,21,22,28]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    # print(reframed.head())

    # split into train and test sets
    values = reframed.values
    n_train_hours = daylight*30*3
    train = values[:n_train_hours, :]
    test = values[n_train_hours:, :]
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]
    
    print('-----------------BEGIN------------------')
    
    # concat column name
    target_col_name = 'var'+ str(a - 15) +'(t)'
    print('target: ' + target_col_name + ' ' + pf_names[pf_name_index])
    pf_name_index += 1
    
    
    # get train and label data and put into feature selector
    train_labels = reframed[target_col_name]
    train = reframed.drop(columns = [target_col_name])
    
    fs = FeatureSelector(data=train, labels=train_labels)
    
    # 1. missing values treshold = 0.6
    print('1. missing values')
    fs.identify_missing(missing_threshold=0.6)
    fs.plot_missing()
    
    # 2. single unique value
    print('2. single unique value')
    fs.identify_single_unique()
    fs.plot_unique()
    
    # 3. colinear features
    print('3. colinear features')
    fs.identify_collinear(correlation_threshold=0.75)
    fs.plot_collinear(plot_all=True)
    print(fs.record_collinear.head())
    
    # 4. zero importance features
    print('4. zero importance')
    fs.identify_zero_importance(task='regression',
                                eval_metric='auc',
                                n_iterations=10,
                                early_stopping=True)
    zero_importance_features = fs.ops['zero_importance']
    fs.plot_feature_importances(threshold=0.99, plot_n=12)
    
    # 5. low importance features
    print('5. low importance features')
    fs.identify_low_importance(cumulative_importance=0.9)
    print(fs.feature_importances.head(16))
    
    print('-----------------END--------------------')

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    # make a prediction
    
    yhat = model.predict(test_X)
    # print(yhat)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    print('-------SHAPE OF INV_YHAT---------')
    print(inv_yhat.shape)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)



df3=pd.read_csv(save_name2, header=0, index_col=0)
model_metrics=df3.values
x_axix=pf_names
mape_br = []
mape_lr = []
mape_etc = []
mape_svr = []
mape_gbr=[]
mape_lstm=[]

j=1
for i in range(len(model_metrics)):
  mape_lstm.append(model_metrics[i][j])
# if i%6==0:
#     mape_br.append(model_metrics[i][j])
#     mape_lr.append(model_metrics[i+1][j])
#     mape_etc.append(model_metrics[i+2][j])
#     mape_svr.append(model_metrics[i+3][j])
#     mape_gbr.append(model_metrics[i+4][j])

plt.figure(figsize=[7.5,5])
plt.rc('font',family='Times New Roman')
# plt.plot(x_axix, mape_br,"o-", color='purple', label=model_names[0])
# plt.plot(x_axix, mape_lr, "o-",color='red', label=model_names[1])
# plt.plot(x_axix, mape_etc, "o-",color='skyblue', label=model_names[2])
# plt.plot(x_axix, mape_svr,"o-", color='blue', label=model_names[3])
# plt.plot(x_axix, mape_gbr, "o-",color='green', label=model_names[4])
plt.plot(x_axix, mape_lstm,"o-", color='orange', label=model_names[5])

plt.legend(fontsize=16,loc='upper right')
plt.xlabel('Physical factors',size=18)
plt.ylabel('MAPE',size=18)
plt.xticks(size=14)
plt.yticks(size=14)
plt.tight_layout()
# plt.savefig(save_png1)
plt.show()

mape_br = []
mape_lr = []
mape_etc = []
mape_svr = []
mape_gbr=[]
mape_lstm=[]
j=2
for i in range(len(model_metrics)):
  mape_lstm.append(model_metrics[i][j])
#     if i%6==0:
#         mape_br.append(model_metrics[i][j])
#         mape_lr.append(model_metrics[i+1][j])
#         mape_etc.append(model_metrics[i+2][j])
#         mape_svr.append(model_metrics[i+3][j])
#         mape_gbr.append(model_metrics[i+4][j])

plt.figure(figsize=[7.5,5])
plt.rc('font',family='Times New Roman')
# plt.plot(x_axix, mape_br,"o-", color='purple', label=model_names[0])
# plt.plot(x_axix, mape_lr, "o-",color='red', label=model_names[1])
# plt.plot(x_axix, mape_etc, "o-",color='skyblue', label=model_names[2])
# plt.plot(x_axix, mape_svr,"o-", color='blue', label=model_names[3])
# plt.plot(x_axix, mape_gbr, "o-",color='green', label=model_names[4])
plt.plot(x_axix, mape_lstm,"o-", color='black', label=model_names[5])

plt.legend(fontsize=16,loc='upper right')
plt.xlabel('Physical factors',size=18)
plt.ylabel('RMSE',size=18)
plt.xticks(size=14)
plt.yticks(size=14)
plt.tight_layout()
# plt.savefig(save_png2)
plt.show()

"""# solution 1: try to drop unnecessary features"""

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=40
evening=65
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
values = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1).values

# frame as supervised learning
reframed = series_to_supervised(values, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]
  
  
# Added by Shen Zhu
# Useless features
useless_feature = [
                    [1, 3, 4, 6, 7, 10],
                    [1, 3, 7, 10],
                    [1, 7],
                    [1, 3, 4, 7, 9],
                    [7],
                    [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15]
                  ]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    reframed.drop(reframed.columns[useless_feature[pf_name_index]], axis=1, inplace=True)
    
    reframed = reframed.values.astype('float32')
    
    scaler = MinMaxScaler(feature_range=(0.000001, 1))
    reframed = scaler.fit_transform(reframed)
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = daylight*30*3
    train = reframed[:n_train_hours, :]
    test = reframed[n_train_hours:, :]
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=0,
                        shuffle=False)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    # make a prediction
    yhat = model.predict(test_X)
    # print(yhat)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    inv_yhat = concatenate((yhat, test_X), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 2: extend day to 8:00am to 8:00pm"""

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
print(df.shape)
values = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1).values

# frame as supervised learning
reframed = series_to_supervised(values, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]
  
  
# Added by Shen Zhu
# Useless features
useless_feature = [
                    [1, 3, 4, 6, 7, 10],
                    [1, 3, 7, 10],
                    [1, 7],
                    [1, 3, 4, 7, 9],
                    [7],
                    [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15]
                  ]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    reframed.drop(reframed.columns[useless_feature[pf_name_index]], axis=1, inplace=True)
    
    reframed = reframed.values.astype('float32')
    
    scaler = MinMaxScaler(feature_range=(0.000001, 1))
    reframed = scaler.fit_transform(reframed)
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = daylight*30*3
    train = reframed[:n_train_hours, :]
    test = reframed[n_train_hours:, :]
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    print(history.history['loss'][-1])
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    # make a prediction
    yhat = model.predict(test_X)
    # print(yhat)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    inv_yhat = concatenate((yhat, test_X), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 3: drop weekend data and extend day to 8:00am to 8:00pm"""

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening) & (df['Day Status'] == 0)]
print(df.shape)
values = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1).values

# frame as supervised learning
reframed = series_to_supervised(values, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]
  
  
# Added by Shen Zhu
# Useless features
useless_feature = [
                    [1, 3, 4, 6, 7, 10],
                    [1, 3, 7, 10],
                    [1, 7],
                    [1, 3, 4, 7, 9],
                    [7],
                    [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15]
                  ]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    reframed.drop(reframed.columns[useless_feature[pf_name_index]], axis=1, inplace=True)
    
    reframed = reframed.values.astype('float32')
    
    scaler = MinMaxScaler(feature_range=(0.000001, 1))
    reframed = scaler.fit_transform(reframed)
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = int(reframed.shape[0] * 0.8)
    print(n_train_hours)
    train = reframed[:n_train_hours, :]
    test = reframed[n_train_hours:, :]
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    print(history.history['loss'][-1])
    pyplot.plot(history.history['val_loss'], label='train')
    print(history.history['val_loss'][-1])
    pyplot.legend()
    pyplot.show()

    # make a prediction
    yhat = model.predict(test_X)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    inv_yhat = concatenate((yhat, test_X), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 4: using sample weights

the sample weights are calculated by this formula: (the median frequency of all class lables) / （number of instances in this class)
"""

import statistics

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
print(df.shape)
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
df = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1)

# category-weights dictionary
cntAct = [212, 236, 0, 98, 980, 452, 202, 915, 0, 1576, 331, 574, 357, 32, 79, 3350, 25742]
categories_int = [1, 2, 3, 4, 5, 6, 71, 72, 73,
                  74, 75, 76, 77, 78, 79, 8, 9]
median = statistics.median(cntAct)
weights = []
for nmb in cntAct:
  if(nmb == 0):
    weights.append(median / 1)
  else:
    weights.append(median / nmb)
cate_weights = dict(zip(categories_int, weights))
print(cate_weights)

sample_weights = []

for index, row in df.iterrows():
  sample_weights.append(cate_weights[int(row['Activity'])])
  
print(len(sample_weights))
  
values = df.values
# scale data to 0-1
scaler = MinMaxScaler(feature_range=(0.000001, 1))
scaled = scaler.fit_transform(values)

# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    
    reframed = reframed.values
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = daylight*30*3
    train = reframed[:n_train_hours, :]
    print('============')
    print(train.shape)
    test = reframed[n_train_hours:, :]
    
    sample_weights = np.array(sample_weights[:n_train_hours])
    print('=============')
    print(len(sample_weights))
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False, sample_weight=sample_weights)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    print(history.history['loss'][-1])
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    # make a prediction
    yhat = model.predict(test_X)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    # print(type(yhat))
    # print(yhat)
    print(yhat.shape)
    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("==================RMSE: ", rmse)
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 4.1: sample weights with dropout"""

import statistics

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
print(df.shape)
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
df = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1)

# category-weights dictionary
cntAct = [212, 236, 0, 98, 980, 452, 202, 915, 0, 1576, 331, 574, 357, 32, 79, 3350, 25742]
categories_int = [1, 2, 3, 4, 5, 6, 71, 72, 73,
                  74, 75, 76, 77, 78, 79, 8, 9]
median = statistics.median(cntAct)
weights = []
for nmb in cntAct:
  if(nmb == 0):
    weights.append(median / 1)
  else:
    weights.append(median / nmb)
cate_weights = dict(zip(categories_int, weights))
print(cate_weights)

sample_weights = []

for index, row in df.iterrows():
  sample_weights.append(cate_weights[int(row['Activity'])])
  
print(len(sample_weights))
  
values = df.values
# scale data to 0-1
scaler = MinMaxScaler(feature_range=(0.000001, 1))
scaled = scaler.fit_transform(values)

# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    
    reframed = reframed.values
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = daylight*30*3
    train = reframed[:n_train_hours, :]
    print('============')
    print(train.shape)
    test = reframed[n_train_hours:, :]
    
    sample_weights = np.array(sample_weights[:n_train_hours])
    print('=============')
    print(len(sample_weights))
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]

    #models

    # model_br = BayesianRidge()
    # model_lr = LinearRegression()
    # model_etc = ElasticNet()
    # model_svr = SVR()
    # model_gbr = GradientBoostingRegressor()
    # model_dic = [model_br, model_lr, model_etc, model_svr, model_gbr]
    #
    # testy_list=[]
    # for i in range (len(test_y)):
    #     testy_list.append([test_y[i]])
    # test_y=np.array(testy_list)
    #
    # for model in model_dic:
    #     yhat2=model.fit(train_X, train_y).predict(test_X)
    #     yhat2_list=[]
    #     for i in range (len(yhat2)):
    #         yhat2_list.append([yhat2[i]])
    #     yhat = np.array(yhat2_list)
    #     # print(yhat)
    #     # print(test_y)
    #     # invert scaling for forecast
    #     inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    #     inv_yhat = scaler.inverse_transform(inv_yhat)
    #     inv_yhat = inv_yhat[:, 0]
    #     # # invert scaling for actual
    #     inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    #     inv_y = scaler.inverse_transform(inv_y)
    #     inv_y = inv_y[:, 0]
    #     # print(inv_y, inv_yhat)
    #     # mae = mean_absolute_error(inv_y, inv_yhat)
    #     mape = mean_absolute_percent_error(inv_y, inv_yhat)
    #     rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    #     # print(model_names[m])
    #     # print('Test MAE: %.3f' % mae)
    #     # print('Test RMSE: %.3f' % rmse)
    #     # print(mae,rmse)
    #     model_metrics.append([pf_names[p],mape,rmse])

    ###rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

    # design network
    model = Sequential()
    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
    model.add(Dropout(0.2))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    # fit network
    history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False, sample_weight=sample_weights)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    print(history.history['loss'][-1])
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    
    # make a prediction
    yhat = model.predict(test_X)
    # print(yhat)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    print(type(yhat))
    print(yhat.shape)
    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 5: same time data(whole year)

a day is divided into 8 parts, each 3 hours, starting from 12am to 12pm
"""

import statistics

# change fall to summer/spring if necessary
season='fall'
# read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
read_name='bradjc5_cal_timeslot_full_nomissing.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'

def generate_metric_save_name(index):
  return "prediction_MAPE_RMSE_pf_fall" + str(index) + ".csv"

# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
df = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1)

length_of_one_part = 12
starting_slot = 0

data_frames = []

# 96 / 12 = 8
for i in range(0, 8):
  end_slot = starting_slot + length_of_one_part - 1
  tmp_frame = df[(df['Time slot'] >= starting_slot) & (df['Time slot'] <= end_slot)]
  data_frames.append(tmp_frame)
  starting_slot += 12

round = 0

for round in range(3, 8):
  data_frame = data_frames[i]
  print("=============ROUNT "+str(round)+" TIME SLOT "+str(12 * round)+"-"+str(12 * round + 11)+"==============")
  
  scaler = MinMaxScaler(feature_range=(0.000001, 1))
  scaled = scaler.fit_transform(data_frame.values)
  
  reframed_for_this_round = series_to_supervised(scaled, 1, 1)
  reframed_for_this_round = DataFrame(reframed_for_this_round)
  
  print(reframed_for_this_round.shape)
  
  model_metrics=[]

  # p is the index for recording which feature we are predicting
  p=0

  if season=='summer_3':
    pf_col=[27,24,22]##summer->list out of range
  else:
    pf_col=[27,24,25,21,22,28]

  pf_name_index = 0

  for a in pf_col:
      print("======Predicting feature no. "+str(p)+"=============")
      ##recover
      drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]

      # drop columns we don't want to predict
      reframed = reframed_for_this_round.copy(deep=True)
      
      drop_list.remove(int(a))
      print(reframed.shape)
      reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)

      reframed = reframed.values

      # print(reframed.head())

      # split into train and test sets
      n_train_hours = int(reframed.shape[0] * 0.8)
      train = reframed[:n_train_hours, :]
      test = reframed[n_train_hours:, :]

      sample_weights = np.array(sample_weights[:n_train_hours])

      # split into input and outputs
      train_X, train_y = train[:, :-1], train[:, -1]
      test_X, test_y = test[:, :-1], test[:, -1]

      # rnn
      # reshape input to be 3D [samples, timesteps, features]
      train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
      test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
      print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

      # design network
      model = Sequential()
      model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
      model.add(Dense(1))
      model.compile(loss='mae', optimizer='adam')
      # fit network
      history = model.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                          shuffle=False)
      # plot history
      pyplot.plot(history.history['loss'], label='train')
      print(history.history['loss'][-1])
      pyplot.plot(history.history['val_loss'], label='test')
      pyplot.legend()
      pyplot.show()

      # make a prediction
      yhat = model.predict(test_X)
      # print(yhat)
      test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
      # invert scaling for forecast
      # print(test_X)
      inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
      inv_yhat = scaler.inverse_transform(inv_yhat)
      inv_yhat = inv_yhat[:, 0]
      ## invert scaling for actual
      test_y = test_y.reshape((len(test_y), 1))
      # print (test_y)
      inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
      inv_y = scaler.inverse_transform(inv_y)
      inv_y = inv_y[:, 0]
      # print(inv_y, inv_yhat)
      pred = pd.DataFrame(inv_yhat)
      pred.to_csv(save_pred, index=True)
      true = pd.DataFrame(inv_y)
      true.to_csv(save_true, index=True)
      # #calculate RMSE
      # mae=mean_absolute_error(inv_y, inv_yhat)
      mape=mean_absolute_percent_error(inv_y, inv_yhat)
      rmse = sqrt(mean_squared_error(inv_y, inv_yhat))

      model_metrics.append([pf_names[p], mape, rmse])
      p+=1

  df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
  df2.to_csv(save_name2, index=True)

"""# solution 6: GRU model"""

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
# save_png1='mape_fall.png'
# save_png2='rmse_fall.png'
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


##no replacing
if season=='summer_3':pf_names = ['Temperature','Humidity','Concentration']
else:pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
names =np.array(['LSTM'])
model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
values = df.drop(['Time','Location','Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1).values
# print(values)
# print(values[:, 4])
# # integer encode direction
# encoder = LabelEncoder()
# values[:, 4] = encoder.fit_transform(values[:, 4])#Activity
# #ensure all data is float
values = values.astype('float32')

# #normalize features
scaler = MinMaxScaler(feature_range=(0.000001, 1))
scaled = scaler.fit_transform(values)

print('-----SIZE OF VALUES-------')
print(DataFrame(values).shape)
print(DataFrame(scaled).shape)

# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv(save_name, index=True)

###20Activity
model_metrics=[]
p=0
if season=='summer_3':pf_col=[27,24,22]##summer->list out of range
else:pf_col=[27,24,25,21,22,28]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    # print(reframed.head())

    # split into train and test sets
    values = reframed.values
    n_train_hours = daylight*30*3
    train = values[:n_train_hours, :]
    test = values[n_train_hours:, :]
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]
    
    # rnn
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
    
    # design network
    model_gru = Sequential()
    model_gru.add(GRU(75, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))
    model_gru.add(GRU(units=30))
    model_gru.add(Dense(units=1))
    model_gru.compile(loss='mae', optimizer='adam')

    # fit network
    history = model_gru.fit(train_X, train_y, epochs=100, batch_size=daylight, validation_data=(test_X, test_y), verbose=2,
                        shuffle=False)
    # plot history
    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='test')
    pyplot.legend()
    pyplot.show()

    # make a prediction
    yhat = model_gru.predict(test_X)
    # print(yhat)
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    print('-------SHAPE OF INV_YHAT---------')
    print(inv_yhat.shape)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

"""# solution 7: most similar past data"""

import statistics

# change fall to summer/spring if necessary
season='fall'
read_name='bradjc5_cal_timeslot_full_nomissing_fall.csv'
save_name="reframed_data_fall.csv"
save_name2="prediction_MAPE_RMSE_pf_fall.csv"
save_pred='pred_pf_lstm_fall.csv'
save_true='true_pf_lstm_fall.csv'


# no replacing
if season=='summer_3':
  pf_names = ['Temperature','Humidity','Concentration']
else: 
  pf_names = ['Temperature','Humidity','Illumination','CO2','Concentration','VOC']
  
names =np.array(['LSTM'])

model_names=np.tile(names,len(pf_names)).tolist()

df = read_csv(read_name, header=0, index_col=0)
morning=32
evening=79
daylight=evening-morning+1
print(df.shape)
df = df[(df['Time slot'] >= morning) & (df['Time slot'] <= evening)]
df = df.drop(['Time', 'Location', 'Weather','Pressure(inHg)','Wind(mph)','MonDay'], 1)

# category-weights dictionary
cntAct = [212, 236, 0, 98, 980, 452, 202, 915, 0, 1576, 331, 574, 357, 32, 79, 3350, 25742]
categories_int = [1, 2, 3, 4, 5, 6, 71, 72, 73,
                  74, 75, 76, 77, 78, 79, 8, 9]
median = statistics.median(cntAct)
weights = []
for nmb in cntAct:
  if(nmb == 0):
    weights.append(median / 1)
  else:
    weights.append(median / nmb)
cate_weights = dict(zip(categories_int, weights))
print(cate_weights)

sample_weights = []

for index, row in df.iterrows():
  sample_weights.append(cate_weights[int(row['Activity'])])
  
print(len(sample_weights))
  
values = df.values
# scale data to 0-1
scaler = MinMaxScaler(feature_range=(0.000001, 1))
scaled = scaler.fit_transform(values)

# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv(save_name, index=True)

# 20Activity
model_metrics=[]

# p is the index for recording which feature we are predicting
p=0
if season=='summer_3':
  pf_col=[27,24,22]##summer->list out of range
else:
  pf_col=[27,24,25,21,22,28]

pf_name_index = 0
for a in pf_col:
    ##recover
    drop_list = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
    reframed = read_csv(save_name, header=0, index_col=0)

    # drop columns we don't want to predict
    drop_list.remove(int(a))
    reframed.drop(reframed.columns[drop_list], axis=1, inplace=True)
    
    reframed = reframed.values
    
    # print(reframed.head())

    # split into train and test sets
    n_train_hours = daylight*30*3
    train = reframed[:n_train_hours, :]
    print('============')
    print(train.shape)
    test = reframed[n_train_hours:, :]
    
    sample_weights = np.array(sample_weights[:n_train_hours])
    print('=============')
    print(len(sample_weights))
    
    
    # split into input and outputs
    train_X, train_y = train[:, :-1], train[:, -1]
    test_X, test_y = test[:, :-1], test[:, -1]
    
    '''
    train_X: training data
    train_y: training result
    test_X: test data
    test_y: test result
    '''
    
    # find the past date with the most similar data and calculate the difference
    pred = []
    for test_line in test_X:
      similarity = []
      for train_line in train_X:
        diff = [abs(train-test) for train, test in zip(train_line, test_line)]
        similarity.append(sum(diff)/len(diff))
      index_min = np.argmin(similarity)
      pred.append([train_y[index_min]])
     
    # print("=============pred: ", len(pred) == len(test_y))
    # calculate the RMSE between true data and the most similar past data
    print("=============RMSE: ", mean_squared_error(test_y, pred, multioutput='uniform_average'))

    yhat = np.asarray(pred, dtype=np.float64)
    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    
    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
    # invert scaling for forecast
    # print(test_X)
    # print(type(yhat))
    print(yhat.shape)
    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:, 0]
    ## invert scaling for actual
    test_y = test_y.reshape((len(test_y), 1))
    # print (test_y)
    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:, 0]
    # print(inv_y, inv_yhat)
    pred = pd.DataFrame(inv_yhat)
    pred.to_csv(save_pred, index=True)
    true = pd.DataFrame(inv_y)
    true.to_csv(save_true, index=True)
    # #calculate RMSE
    # mae=mean_absolute_error(inv_y, inv_yhat)
    mape=mean_absolute_percent_error(inv_y, inv_yhat)
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
    # print("LSTM")
    # print('Test MAE: %.3f' % mae)
    # print('Test RMSE: %.3f' % rmse)
    # print("LSTM", mae, rmse)
    model_metrics.append([pf_names[p], mape, rmse])
    p+=1

df2 = pd.DataFrame(model_metrics, index=model_names, columns=["Physical factors","MAPE","RMSE"])
df2.to_csv(save_name2, index=True)

